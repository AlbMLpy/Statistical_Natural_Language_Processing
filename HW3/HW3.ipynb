{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuopDBVAbXw0"
      },
      "source": [
        "# 1. Information about the submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpA7WTQhbXw1"
      },
      "source": [
        "## 1.1 Name and number of the assignment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9huxl9PbXw1"
      },
      "source": [
        "## **Taxonomy enrichment**, Assignment 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goD8WKDWbXw1"
      },
      "source": [
        "## 1.2 Student name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m3IGp7jbXw2"
      },
      "source": [
        "## **Albert Sayapin**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H0hd-0nbXw2"
      },
      "source": [
        "## 1.3 Codalab user ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iMH-uncbXw2"
      },
      "source": [
        "## **albertSayapin**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K25kZmnbXw2"
      },
      "source": [
        "## 1.4 Additional comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc-cAamgbXw2"
      },
      "source": [
        "## *This is an interesting task, but I wish I had more spare time to feel it thoroughly:(*\n",
        "\n",
        "Checked it on **Google Colab** successfully!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4KDDqm7bXw2"
      },
      "source": [
        "# 2. Technical Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3QgTY-JbXw3"
      },
      "source": [
        "## 2.1 Methodology "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA2F9Ht_bXw3"
      },
      "source": [
        "The main problem I tried to solve is **Taxonomy enrichment** task, meaning that we want to *add new unseen words to the taxonomy* and match them with the hypernyms from the existing set.\n",
        "\n",
        "Identifying hypernymic relations has a lot of applications in Natural Language Processing, especially in semantically intensive tasks, such as *Question Answering*, *Textual\n",
        "Entailment*, and *semantic search systems*. These relations play a crucial role in thesaurus\n",
        "construction, but it is challenging and not effective to extract them manually. That is why the problem is worth solving.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "*Existing taxonomy*: \"Mona Lisa\" is-a {\"Painting\", \"Art\", \"Picture\", ...}, \"Equation\" is-a {\"Mathematical object\", \"Representation\", \"Math\", ...}\n",
        "\n",
        "*Add word to the taxonomy*: \"Square\" -> is-a {\"Math\", \"Representation\", ..., \"Painting\"}\n",
        "\n",
        "\n",
        "Speaking of a model I used, actually it is a changed baseline:\n",
        "- Calculate embeddings of all the words in the synset and average them;\n",
        "- Calculate embeddings of out of taxonomy(orphans) words;\n",
        "- Calculate similarity scores between all the synsets and orphans and find 10-15 the most similar(cosine similarity, dot product);\n",
        "- Return top-10 hypernyms of the corresponding synsets.\n",
        "\n",
        "However, I used diferent pretrained embeddings like http://vectors.nlpl.eu/repository/20/214.zip rather than https://dl.fbaipublicfiles.com/'fasttext/vectors-crawl/cc.ru.300.bin.gz, because could not run them neither in colab nor on a local laptoop(loaded too much RAM)\n",
        "\n",
        "\n",
        "As a methodology resource I used these papers:\n",
        "- https://www.dialog-21.ru/media/5111/nikishinaiplusetal-160.pdf\n",
        "- https://www.dialog-21.ru/media/5123/tikhomirovmmplusetal-149.pdf\n",
        "- https://www.dialog-21.ru/media/5125/yadrintsevvvplusetal-144.pdf\n",
        "\n",
        "Steps of the project:\n",
        "\n",
        "1. **Data preprocessing**: I had to preprocess \"context\" column of every dataset(train/test):\n",
        "- *Lemmatized* all the words by pymystem3.Mystem stemmer and made them lowercase(Normalization step);\n",
        "- *Dropped* all the words from nltk *Russian stopwords* list(As they do not bring any additional information);\n",
        "- *Eliminated* all the words with *length* less than 2;\n",
        "\n",
        "2. **Model training**: I had to calculate the embeddings of all the synsets in a RuWordNet with:\n",
        "- geowac_lemmas_none_fasttextskipgram_300_5_2020\n",
        "- ruscorpora_upos_skipgram_300_5_2018\n",
        "- normalized or not.\n",
        "\n",
        "3. **Model evaluation**: I had to use *MRR* and *MAP* to get intuition about the model quality.\n",
        "\n",
        "4. **Send the results**: the test.tsv -> .zip files to CodaLab system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3HhXJQ9bXw3"
      },
      "source": [
        "## 2.2 Discussion of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcd86q8abXw3"
      },
      "source": [
        "### **Summary of the experiment:**\n",
        "Here below you can the best results I achieved with the help of:\n",
        "- **TE20** -> which is based on *geowac_lemmas_none_fasttextskipgram_300_5_2020*\n",
        "- **TE20N** -> which is normalized version of **TE20**\n",
        "- **TE18** -> which is based on *ruscorpora_upos_skipgram_300_5_2018*\n",
        "- **TE18N** -> which is normalized version of **TE18**\n",
        "\n",
        "### *Train.csv results:*\n",
        "\n",
        "Method | Nouns_Public MAP | Nouns_Public MRR | Verbs_Public MAP| Verbs_Public MRR|\n",
        "--- | --- | --- | --- | --- |\n",
        "TE20 | 0.17 | 0.18 | 0.08 | 0.09 |\n",
        "TE20N | 0.21 | 0.24 | 0.09 | 0.10 |\n",
        "TE18 | 0.20  | 0.22 | 0.09 | 0.11 |\n",
        "TE18N | 0.23 | 0.25 | 0.12 | 0.14 |\n",
        "Baseline | 0.42 | 0.45 | 0.33 | 0.38 | \n",
        "\n",
        "**Remark:** results for the baseline are from private evaluation(they are likely not so different from the public one)\n",
        "\n",
        "\n",
        "The tables show us that the model based on *geowac_lemmas_none_fasttextskipgram_300_5_2020* and *ruscorpora_upos_skipgram_300_5_2018* works worse than model with embeddings from fasttext/vectors-crawl.\n",
        "This is a direct problem of embeddings that do not reflect the senses for the particular data.\n",
        "That is why we can conclude that embeddings are a key component if we prefer to use this methodology.\n",
        "\n",
        "### **Conclusion:**\n",
        "As we can see from the results a model which is based on pretrained word embeddings can work pretty well(Baseline)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZJRdRr1bXw4"
      },
      "source": [
        "# 3. Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKRayh2FbXw4"
      },
      "source": [
        "## 3.1 Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llKIoxPKbXw4"
      },
      "source": [
        "# Some essential packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhRJ3ceigVF-",
        "outputId": "97027c1a-3a6f-44de-e389-f8e531119fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymystem3==0.1.10\n",
            "  Downloading pymystem3-0.1.10-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pymystem3==0.1.10) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (3.0.4)\n",
            "Installing collected packages: pymystem3\n",
            "  Attempting uninstall: pymystem3\n",
            "    Found existing installation: pymystem3 0.2.0\n",
            "    Uninstalling pymystem3-0.2.0:\n",
            "      Successfully uninstalled pymystem3-0.2.0\n",
            "Successfully installed pymystem3-0.1.10\n",
            "Collecting gensim==4.1.2\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.1.2\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# Some essential packages:\n",
        "!pip install pymystem3==0.1.10\n",
        "!pip install gensim==4.1.2\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afjE6kUsbXw4"
      },
      "source": [
        "## 3.2 Download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQIIXmCbbXw4",
        "outputId": "57a6851c-04b7-4525-a5fd-ef1cf876c2eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-24 08:15:06--  http://vectors.nlpl.eu/repository/20/214.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1920218982 (1.8G) [application/zip]\n",
            "Saving to: ‘214.zip’\n",
            "\n",
            "214.zip             100%[===================>]   1.79G  94.9MB/s    in 19s     \n",
            "\n",
            "2021-12-24 08:15:25 (95.8 MB/s) - ‘214.zip’ saved [1920218982/1920218982]\n",
            "\n",
            "Archive:  214.zip\n",
            "  inflating: ru_fasttext_model/meta.json  \n",
            "  inflating: ru_fasttext_model/model.model  \n",
            "  inflating: ru_fasttext_model/model.model.vectors_ngrams.npy  \n",
            "  inflating: ru_fasttext_model/model.model.vectors.npy  \n",
            "  inflating: ru_fasttext_model/model.model.vectors_vocab.npy  \n",
            "  inflating: ru_fasttext_model/README  \n",
            "--2021-12-24 08:16:15--  http://vectors.nlpl.eu/repository/20/213.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1485270300 (1.4G) [application/zip]\n",
            "Saving to: ‘213.zip’\n",
            "\n",
            "213.zip             100%[===================>]   1.38G  79.0MB/s    in 30s     \n",
            "\n",
            "2021-12-24 08:16:45 (46.5 MB/s) - ‘213.zip’ saved [1485270300/1485270300]\n",
            "\n",
            "Archive:  213.zip\n",
            "  inflating: new_model/meta.json     \n",
            "  inflating: new_model/model.model   \n",
            "  inflating: new_model/model.model.vectors_ngrams.npy  \n",
            "  inflating: new_model/model.model.vectors.npy  \n",
            "  inflating: new_model/model.model.vectors_vocab.npy  \n",
            "  inflating: new_model/README        \n",
            "--2021-12-24 08:17:27--  https://github.com/dialogue-evaluation/taxonomy-enrichment/archive/refs/heads/master.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/dialogue-evaluation/taxonomy-enrichment/zip/refs/heads/master [following]\n",
            "--2021-12-24 08:17:27--  https://codeload.github.com/dialogue-evaluation/taxonomy-enrichment/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.121.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.121.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [     <=>            ]  17.69M  13.6MB/s    in 1.3s    \n",
            "\n",
            "2021-12-24 08:17:29 (13.6 MB/s) - ‘master.zip’ saved [18552090]\n",
            "\n",
            "Archive:  master.zip\n",
            "6ee12174163368f38276b6e58e81c21625660e13\n",
            "   creating: tax_rich/taxonomy-enrichment-master/\n",
            "  inflating: tax_rich/taxonomy-enrichment-master/.gitignore  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/LICENSE  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/README.md  \n",
            "   creating: tax_rich/taxonomy-enrichment-master/baselines/\n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/README.md  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/bert_context_vectorizer.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/bert_initial_vectorizer.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/bert_model.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/fasttext_vectorizer.py  \n",
            "   creating: tax_rich/taxonomy-enrichment-master/baselines/helpers/\n",
            " extracting: tax_rich/taxonomy-enrichment-master/baselines/helpers/__init__.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/helpers/lemmatize_ud.bash  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/helpers/news_corpus_reader.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/helpers/texts_extractor.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/helpers/tf2pytorch_converter.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/main.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/predict_models.py  \n",
            "   creating: tax_rich/taxonomy-enrichment-master/baselines/ruwordnet/\n",
            " extracting: tax_rich/taxonomy-enrichment-master/baselines/ruwordnet/__init__.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/ruwordnet/database.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/baselines/ruwordnet/ruwordnet_reader.py  \n",
            "   creating: tax_rich/taxonomy-enrichment-master/data/\n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/README.md  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/get_reference_format.py  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/get_wordnet_trainset.ipynb  \n",
            "   creating: tax_rich/taxonomy-enrichment-master/data/private_test/\n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/private_test/nouns_private.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/private_test/verbs_private.tsv  \n",
            "   creating: tax_rich/taxonomy-enrichment-master/data/public_test/\n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/public_test/nouns_public.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/public_test/verbs_public.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/ruwordnet.zip  \n",
            "   creating: tax_rich/taxonomy-enrichment-master/data/sample_answers/\n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/sample_answers/nouns.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/sample_answers/verbs.tsv  \n",
            "   creating: tax_rich/taxonomy-enrichment-master/data/training_data/\n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/all_data_nouns.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/all_data_verbs.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/dev_nouns.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/dev_verbs.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/synsets_nouns.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/synsets_verbs.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/test_nouns.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/test_verbs.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/training_nouns.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/data/training_data/training_verbs.tsv  \n",
            "  inflating: tax_rich/taxonomy-enrichment-master/evaluate.py  \n"
          ]
        }
      ],
      "source": [
        "# Embeddings for the model:\n",
        "!wget http://vectors.nlpl.eu/repository/20/214.zip\n",
        "!unzip -o 214.zip -d ru_fasttext_model\n",
        "!rm 214.zip\n",
        "\n",
        "!wget http://vectors.nlpl.eu/repository/20/213.zip\n",
        "!unzip -o 213.zip -d new_model\n",
        "!rm 213.zip\n",
        "\n",
        "!wget https://github.com/dialogue-evaluation/taxonomy-enrichment/archive/refs/heads/master.zip\n",
        "!unzip -o master.zip -d tax_rich\n",
        "!rm master.zip\n",
        "\n",
        "# Create directory for the results:\n",
        "!mkdir results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR96tKtBbXw5",
        "outputId": "45dcc206-d0e2-4adb-ce08-767878b5737c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "russian_stopwords = stopwords.words(\"russian\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqb2CECObXw5"
      },
      "source": [
        "# Load the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sDzoPts4bXw5"
      },
      "outputs": [],
      "source": [
        "path = \"tax_rich/taxonomy-enrichment-master/data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XeFHndNbXw5"
      },
      "source": [
        "## Nouns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6co1Mbs1bXw5"
      },
      "outputs": [],
      "source": [
        "part = 'nouns'\n",
        "\n",
        "# for model:\n",
        "synsets_nouns = pd.read_csv(path + f'training_data/synsets_{part}.tsv', sep='\\t', encoding='utf-8')\n",
        "synsets_nouns[\"PARENTS\"] = synsets_nouns[\"PARENTS\"].apply(lambda x: ast.literal_eval(x))\n",
        "synsets_nouns[\"PARENT_TEXTS\"] = synsets_nouns[\"PARENT_TEXTS\"].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "# public data:\n",
        "nouns_public = pd.read_csv(path + f'public_test/{part}_public.tsv', sep='\\t', names=[\"TEXT\"])\n",
        "\n",
        "# private data:\n",
        "nouns_private = pd.read_csv(path + f'private_test/{part}_private.tsv', sep='\\t', names=[\"TEXT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyRD9HcdbXw6"
      },
      "source": [
        "## Verbs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k42lqO5vbXw6"
      },
      "outputs": [],
      "source": [
        "part = 'verbs'\n",
        "\n",
        "# for model:\n",
        "synsets_verbs = pd.read_csv(path + f'training_data/synsets_{part}.tsv', sep='\\t', encoding='utf-8')\n",
        "synsets_verbs[\"PARENTS\"] = synsets_verbs[\"PARENTS\"].apply(lambda x: ast.literal_eval(x))\n",
        "synsets_verbs[\"PARENT_TEXTS\"] = synsets_verbs[\"PARENT_TEXTS\"].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "# public data:\n",
        "verbs_public = pd.read_csv(path + f'public_test/{part}_public.tsv', sep='\\t', names=[\"TEXT\"])\n",
        "\n",
        "# private data:\n",
        "verbs_private = pd.read_csv(path + f'private_test/{part}_private.tsv', sep='\\t', names=[\"TEXT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJbCSTIfbXw6"
      },
      "source": [
        "# Preprocess the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xEzdz2EObXw6"
      },
      "outputs": [],
      "source": [
        "def lemmatized_context(row, stemmer):\n",
        "    s = row[\"TEXT\"]\n",
        "    tokens = stemmer.lemmatize(s.lower())\n",
        "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
        "                and token != \" \" \\\n",
        "                and re.match('[\\w\\-]+$', token)\\\n",
        "                and (len(token) > 1)\n",
        "            ]\n",
        "    return tokens\n",
        "\n",
        "def get_counter(row):\n",
        "    s = row['context']\n",
        "    c = Counter(s)\n",
        "    return list(set(s)), c\n",
        "\n",
        "def get_counter_global(data):\n",
        "    return data.groupby(\"word\").apply(lambda x: Counter(x[\"context\"].sum()))\n",
        "\n",
        "def preprocess(data):\n",
        "    stemmer = Mystem()\n",
        "    data['tokens'] = data.apply(lemmatized_context, 1, stemmer=stemmer)\n",
        "    return data\n",
        "\n",
        "def get_hypernyms(x, y_df):\n",
        "    res_list = []\n",
        "    res_set = set()\n",
        "    cand = np.argsort([x.dot(y) for y in y_df[\"emb\"]])[::-1][:10]\n",
        "    hts = y_df.iloc[cand][\"PARENTS\"].tolist()\n",
        "    for cd in hts:\n",
        "        for j in cd:\n",
        "            if len(res_list) == 10:\n",
        "                break\n",
        "            if j not in res_set:\n",
        "                res_set.add(j)\n",
        "                res_list.append(j)\n",
        "    return res_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQL5-zSHbXw6",
        "outputId": "5b607d16-e199-40e6-eb81-3d19df92c3c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n"
          ]
        }
      ],
      "source": [
        "preprocess(synsets_nouns)\n",
        "preprocess(synsets_verbs)\n",
        "\n",
        "preprocess(nouns_public)\n",
        "preprocess(nouns_private)\n",
        "\n",
        "preprocess(verbs_public)\n",
        "preprocess(verbs_private);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_9cdQ0XbXw6"
      },
      "source": [
        "# Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oRZNKSMabXw6"
      },
      "outputs": [],
      "source": [
        "def get_hypernyms(x, y_df):\n",
        "    res_list = []\n",
        "    res_set = set()\n",
        "    cand = np.argsort([x.dot(y) for y in y_df[\"emb\"]])[::-1][:10]\n",
        "    hts = y_df.iloc[cand][\"PARENTS\"].tolist()\n",
        "    for cd in hts:\n",
        "        for j in cd:\n",
        "            if len(res_list) == 10:\n",
        "                break\n",
        "            if j not in res_set:\n",
        "                res_set.add(j)\n",
        "                res_list.append(j)\n",
        "    return res_list\n",
        "\n",
        "def save_to_file(file_name, df):\n",
        "    with open(file_name, \"w\") as f:\n",
        "        for i in range(len(df)):\n",
        "            hypernyms = df.iloc[i][\"hypernyms\"]\n",
        "            for j in hypernyms:\n",
        "                f.write(f\"{df.iloc[i]['TEXT']}\\t{j}\\n\")\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eLkRoKjWbXw6"
      },
      "outputs": [],
      "source": [
        "emb_18 = 'ru_fasttext_model/model.model'\n",
        "emb_20 = 'new_model/model.model'\n",
        "choice = emb_20\n",
        "\n",
        "model = KeyedVectors.load(choice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDyTeI0YbXw6"
      },
      "source": [
        "## Count embeddings of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9-YIalIJbXw6"
      },
      "outputs": [],
      "source": [
        "def count_embeddings(x, model):\n",
        "    res = model[x].mean(axis=0)\n",
        "    return res / np.linalg.norm(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Jrl3DSs7bXw7"
      },
      "outputs": [],
      "source": [
        "# original:\n",
        "#synsets_nouns[\"emb\"] = synsets_nouns[\"tokens\"].apply(lambda x: model[x].mean(axis=0))\n",
        "#synsets_verbs[\"emb\"] = synsets_verbs[\"tokens\"].apply(lambda x: model[x].mean(axis=0))\n",
        "\n",
        "# normalized:\n",
        "synsets_nouns[\"emb\"] = synsets_nouns[\"tokens\"].apply(count_embeddings, model=model)\n",
        "synsets_verbs[\"emb\"] = synsets_verbs[\"tokens\"].apply(count_embeddings, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4OwkRWSbXw7"
      },
      "source": [
        "## Public Nouns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMYdokaabXw7",
        "outputId": "373879a8-924f-42fa-b219-83a8e71f0e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: nouns_public_results.tsv (deflated 81%)\n"
          ]
        }
      ],
      "source": [
        "# original:\n",
        "#nouns_public[\"emb\"] = nouns_public[\"tokens\"].apply(lambda x: model[x].mean(axis=0))\n",
        "\n",
        "# normalized:\n",
        "nouns_public[\"emb\"] = nouns_public[\"tokens\"].apply(count_embeddings, model=model)\n",
        "\n",
        "nouns_public[\"hypernyms\"] = nouns_public[\"emb\"].apply(get_hypernyms, y_df=synsets_nouns)\n",
        "\n",
        "file_name = \"nouns_public_results.tsv\"\n",
        "save_to_file(file_name, nouns_public)\n",
        "\n",
        "!zip results/nouns_public_results.zip nouns_public_results.tsv\n",
        "!rm nouns_public_results.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iemixiXFbXw7"
      },
      "source": [
        "## Private Nouns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruGIyWhFbXw7",
        "outputId": "2b6fe11a-115c-41d8-e50a-e75fcbc0870e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: nouns_private_results.tsv (deflated 82%)\n"
          ]
        }
      ],
      "source": [
        "# original:\n",
        "#nouns_private[\"emb\"] = nouns_private[\"tokens\"].apply(lambda x: model[x].mean(axis=0))\n",
        "\n",
        "# normalized:\n",
        "nouns_private[\"emb\"] = nouns_private[\"tokens\"].apply(count_embeddings, model=model)\n",
        "\n",
        "nouns_private[\"hypernyms\"] = nouns_private[\"emb\"].apply(get_hypernyms, y_df=synsets_nouns)\n",
        "\n",
        "file_name = \"nouns_private_results.tsv\"\n",
        "save_to_file(file_name, nouns_private)\n",
        "\n",
        "!zip results/nouns_private_results.zip nouns_private_results.tsv\n",
        "!rm nouns_private_results.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VFDc6nBbXw7"
      },
      "source": [
        "## Public Verbs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEhYY50CbXw7",
        "outputId": "93fc0a54-b2c4-408e-be33-01239a0a010a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: verbs_public_results.tsv (deflated 84%)\n"
          ]
        }
      ],
      "source": [
        "# original:\n",
        "#verbs_public[\"emb\"] = verbs_public[\"tokens\"].apply(lambda x: model[x].mean(axis=0))\n",
        "\n",
        "# normalized:\n",
        "verbs_public[\"emb\"] = verbs_public[\"tokens\"].apply(count_embeddings, model=model)\n",
        "\n",
        "verbs_public[\"hypernyms\"] = verbs_public[\"emb\"].apply(get_hypernyms, y_df=synsets_verbs)\n",
        "\n",
        "file_name = \"verbs_public_results.tsv\"\n",
        "save_to_file(file_name, verbs_public)\n",
        "\n",
        "!zip results/verbs_public_results.zip verbs_public_results.tsv\n",
        "!rm verbs_public_results.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPO7UyFmbXw7"
      },
      "source": [
        "## Private Verbs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SSRg1LibXw7",
        "outputId": "8162278e-c76b-4678-9ee3-ca36a1d9fc0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: verbs_private_results.tsv (deflated 84%)\n"
          ]
        }
      ],
      "source": [
        "# original:\n",
        "#verbs_private[\"emb\"] = verbs_private[\"tokens\"].apply(lambda x: model[x].mean(axis=0))\n",
        "\n",
        "# normalized:\n",
        "verbs_private[\"emb\"] = verbs_private[\"tokens\"].apply(count_embeddings, model=model)\n",
        "\n",
        "verbs_private[\"hypernyms\"] = verbs_private[\"emb\"].apply(get_hypernyms, y_df=synsets_verbs)\n",
        "\n",
        "file_name = \"verbs_private_results.tsv\"\n",
        "save_to_file(file_name, verbs_private)\n",
        "\n",
        "!zip results/verbs_private_results.zip verbs_private_results.tsv\n",
        "!rm verbs_private_results.tsv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "34002bb53b7508659aadbc48d2690eaa0fedfc1e7a06e0392af55e4acdbf6811"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit ('snlp': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
