{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0",
      "metadata": {
        "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
      },
      "source": [
        "# 1. Information about the submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3",
      "metadata": {
        "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3"
      },
      "source": [
        "## 1.1 Name and number of the assignment "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9",
      "metadata": {
        "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9"
      },
      "source": [
        "## **Word Sense Induction(Knowledge-free)**, Assignment 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d",
      "metadata": {
        "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d"
      },
      "source": [
        "## 1.2 Student name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2",
      "metadata": {
        "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2"
      },
      "source": [
        "## **Albert Sayapin**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a46ab45-d215-41af-b910-63ff4a215a07",
      "metadata": {
        "id": "8a46ab45-d215-41af-b910-63ff4a215a07"
      },
      "source": [
        "## 1.3 Codalab user ID"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a",
      "metadata": {
        "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a"
      },
      "source": [
        "## **albertSayapin**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70456c74-8e1f-4da0-bebe-fbceee169115",
      "metadata": {
        "id": "70456c74-8e1f-4da0-bebe-fbceee169115"
      },
      "source": [
        "## 1.4 Additional comments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b810ac6-7739-4f7f-8bea-dbf1198570ea",
      "metadata": {
        "id": "6b810ac6-7739-4f7f-8bea-dbf1198570ea"
      },
      "source": [
        "## *This is a very cool task:)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af498ab-3c00-4d36-a962-c947862fede8",
      "metadata": {
        "id": "1af498ab-3c00-4d36-a962-c947862fede8"
      },
      "source": [
        "# 2. Technical Report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061f71b9-114a-4cb0-b531-5711970317bf",
      "metadata": {
        "id": "061f71b9-114a-4cb0-b531-5711970317bf"
      },
      "source": [
        "## 2.1 Methodology "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3",
      "metadata": {
        "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3"
      },
      "source": [
        "The main problem I tried to solve is **Word Sense Induction**, meaning that we want to automatically discover the senses of semantically ambiguous words from unannotated text.\n",
        "\n",
        "Generally speaking, I used the model which consists of 3 components(Knowledge-Free):\n",
        "- precalculated words embeddings;\n",
        "- particular weighting scheme of embeddings;\n",
        "- 1 step / 2 step clustering technique, namely how many clustering algorithms I use to get better results;\n",
        "\n",
        "There are some essential steps in this project I had to walk through:\n",
        "\n",
        "1. **Data preprocessing**: I had to preprocess \"context\" column of every dataset(train/test):\n",
        "- *Lemmatized* all the words by pymystem3.Mystem stemmer and made them lowercase(Normalization step);\n",
        "- *Dropped* all the words from nltk *Russian stopwords* list(As they do not bring any information usually);\n",
        "- *Eliminated* all the words with *length* less than 3 and target word(\"word\" column) as well;\n",
        "- *Calculated words occurrences* both locally(for every context for every word) and globally(for every word) and left only unique tokens;\n",
        "(It could help to get more elaborate weighting method)\n",
        "\n",
        "2. **Model training**: I had to find optimal parameters for my model(using Brute Force as the size of the problem is not that big):\n",
        "- *weighting scheme*: Average, Sum, Local Sum, Global Sum;\n",
        "- *Normalize* context vectors or not?\n",
        "- *1 step or 2 step clustering*? (Used AffinityPropagation for the first step to identify the number of clusters)\n",
        "- What *clustering algorithm* to use? (AgglomerativeClustering, AffinityPropagation, KMeans, SpectralClustering)\n",
        "\n",
        "3. **Model evaluation**: I had to use *Adjusted Rank Score* to compare different clusterings.\n",
        "\n",
        "4. **Send the results**: the test.csv -> .zip files to CodaLab system.\n",
        "\n",
        "Some words about why I decided to move this particular way:\n",
        "\n",
        "*First of all*, I used fastText Skipgram model that was precalculated on GeoWAC data. It is a good choice to get results quickly and with high accuracy(especially Gensim interface). \n",
        "\n",
        "*Secondly*, in order to represent context out of words you have to combine them in some way to get one representation.\n",
        "It is not pretty intuitive what method to use: just take the average or sum them up, weight all the words according to their relative frequency. Hence, I decided to look at them all and choose the best.\n",
        "\n",
        "*Thirdly*, It is clustering of course. The most challenging part(and important) I think, as we do not know how to define the number of clusters beforehand. Thanks to Affinity Propagation algorithm, it can find it almost automatically. After that we can just leave it as it is or use the next clustering algorithm that needs the number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe27e49-10c7-4c12-adea-48b0a05a5681",
      "metadata": {
        "id": "afe27e49-10c7-4c12-adea-48b0a05a5681"
      },
      "source": [
        "## 2.2 Discussion of results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b1c84c-c261-46b5-a009-0f2bc4002752",
      "metadata": {
        "id": "b5b1c84c-c261-46b5-a009-0f2bc4002752"
      },
      "source": [
        "***Enter here** a discussion of results and a summary of the experiment. Here we want to see the final table with comparison of the baseline and all tried approaches you decided to report. Even if some method did not bring you to the top of the leaderboard, you should nevertheless indicate this result and a discussion, why, in your opinion, some approach worked and another failed. Interesting findings in the discussion will be a plus.*\n",
        "\n",
        "### Summary of the experiment:\n",
        "\n",
        "### Train.csv results:\n",
        "\n",
        "Method | ARS_wiki-wiki | ARS_bts-rnc | ARS_active-dict|\n",
        "--- | --- | --- | --- |\n",
        "Baseline | 0.6278 | 0.2624 | 0.1764 |\n",
        "WSI_?_?_?_? | ? | ? | ? |\n",
        "WSI_?_?_?_? | ? | ? | ? |\n",
        "\n",
        "\n",
        "### Test.csv results:\n",
        "\n",
        "Method | ARS_wiki-wiki | ARS_bts-rnc | ARS_active-dict|\n",
        "--- | --- | --- | --- |\n",
        "Baseline | 0.6278 | 0.2624 | 0.1764 |\n",
        "WSI_?_?_?_? | ? | ? | ? |\n",
        "WSI_?_?_?_? | ? | ? | ? |\n",
        "\n",
        "### Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe",
      "metadata": {
        "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe"
      },
      "source": [
        "# 3. Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff93e37-3a24-40ab-87db-16b537aad3f6",
      "metadata": {
        "id": "dff93e37-3a24-40ab-87db-16b537aad3f6"
      },
      "source": [
        "## 3.1 Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "73daa932-114b-4e28-9141-13b57c729435",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73daa932-114b-4e28-9141-13b57c729435",
        "outputId": "f34b724f-b4f9-497a-adf9-4e545ae8e773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymystem3==0.1.10 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (0.1.10)\n",
            "Requirement already satisfied: requests in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from pymystem3==0.1.10) (2.26.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests->pymystem3==0.1.10) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests->pymystem3==0.1.10) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests->pymystem3==0.1.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests->pymystem3==0.1.10) (2021.10.8)\n",
            "Requirement already satisfied: gensim in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (4.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gensim) (1.21.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gensim) (1.7.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: sklearn in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from scikit-learn->sklearn) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.21.4)\n",
            "Requirement already satisfied: pandas in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (1.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from pandas) (2021.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from pandas) (1.21.4)\n",
            "Requirement already satisfied: six>=1.5 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
            "Requirement already satisfied: nltk in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (3.6.5)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from nltk) (2021.11.10)\n",
            "Requirement already satisfied: tqdm in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: joblib in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from nltk) (8.0.3)\n",
            "Requirement already satisfied: wget in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (3.2)\n",
            "--2021-11-26 19:20:37--  http://vectors.nlpl.eu/repository/20/214.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1920218982 (1,8G) [application/zip]\n",
            "Saving to: ‘214.zip’\n",
            "\n",
            "214.zip             100%[===================>]   1,79G  1,67MB/s    in 17m 22s \n",
            "\n",
            "2021-11-26 19:37:59 (1,76 MB/s) - ‘214.zip’ saved [1920218982/1920218982]\n",
            "\n",
            "Archive:  214.zip\n",
            "  inflating: ru_fasttext_model/meta.json  \n",
            "  inflating: ru_fasttext_model/model.model  \n",
            "  inflating: ru_fasttext_model/model.model.vectors_ngrams.npy  \n",
            "  inflating: ru_fasttext_model/model.model.vectors.npy  \n",
            "  inflating: ru_fasttext_model/model.model.vectors_vocab.npy  \n",
            "  inflating: ru_fasttext_model/README  \n"
          ]
        }
      ],
      "source": [
        "# Some essential packages:\n",
        "!pip install pymystem3==0.1.10\n",
        "!pip install gensim\n",
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install wget\n",
        "\n",
        "# Embeddings for the model:\n",
        "!wget http://vectors.nlpl.eu/repository/20/214.zip\n",
        "!unzip 214.zip -d ru_fasttext_model\n",
        "!rm 214.zip\n",
        "!mkdir results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9",
      "metadata": {
        "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9"
      },
      "source": [
        "## 3.2 Download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "A5ptzz9XTxCL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ptzz9XTxCL",
        "outputId": "0cae24b3-ccbb-46e6-d6c7-73656cc46ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'russe-wsi-kit'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 148 (delta 4), reused 22 (delta 4), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (148/148), 3.83 MiB | 8.02 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "# To get train/test.csv:\n",
        "!git clone https://github.com/nlpub/russe-wsi-kit.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791dc0e7-337d-46ad-96a3-543a732f19e2",
      "metadata": {
        "id": "791dc0e7-337d-46ad-96a3-543a732f19e2"
      },
      "source": [
        "## 3.3 Data Preprocessing: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "g3PpbSXNWyYP",
      "metadata": {
        "id": "g3PpbSXNWyYP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/albert/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "\n",
        "def lemmatized_context(row, stemmer):\n",
        "    s = row['context']\n",
        "    target = row['word']\n",
        "    \n",
        "    tokens = stemmer.lemmatize(s.lower())\n",
        "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
        "                and token != \" \" \\\n",
        "                and re.match('[\\w\\-]+$', token)\\\n",
        "                and (len(token) > 2)\\\n",
        "                and token != target]\n",
        "    return tokens\n",
        "\n",
        "def get_counter(row):\n",
        "    s = row['context']\n",
        "    c = Counter(s)\n",
        "    return list(set(s)), c\n",
        "\n",
        "def get_counter_global(data):\n",
        "    return data.groupby(\"word\").apply(lambda x: Counter(x[\"context\"].sum()))\n",
        "\n",
        "def read_preprocess(data_path):\n",
        "    \n",
        "    # Read the data:\n",
        "    data = pd.read_csv(data_path, sep='\\t')\n",
        "\n",
        "    # Essential columns:\n",
        "    cols = [\n",
        "        'context_id', 'word', 'gold_sense_id',\n",
        "        'predict_sense_id', 'context',\n",
        "    ]\n",
        "\n",
        "    # Leave only essential columns:\n",
        "    data = data[cols]\n",
        "    \n",
        "    # Lemmatization:\n",
        "    stemmer = Mystem()\n",
        "    data['context'] = data.apply(lemmatized_context, 1, stemmer=stemmer)\n",
        "\n",
        "    # Leave unique and count all:\n",
        "    data[\"embedding_need\"] = data.apply(get_counter, axis=1)\n",
        "\n",
        "    # Calculate frequency based on all contexts:\n",
        "    word2glob_freq = get_counter_global(data).to_dict()\n",
        "\n",
        "    return data, word2glob_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_id</th>\n",
              "      <th>word</th>\n",
              "      <th>gold_sense_id</th>\n",
              "      <th>predict_sense_id</th>\n",
              "      <th>context</th>\n",
              "      <th>embedding_need</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[владимир, мономах, любеч, многочисленный, укр...</td>\n",
              "      <td>([ограда, строительство, лишь, часть, оборонит...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[шильонский, шильйон, известный, русскоязычный...</td>\n",
              "      <td>([шильонский, разный, время, русскоязычный, эл...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[проведение, архитектурный, археологический, р...</td>\n",
              "      <td>([музей, консультация, исторический, управлени...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[топь, белокуров, легенда, завещание, мавра, ю...</td>\n",
              "      <td>([топь, рождение, янтарный, мавра, завещание, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[великий, князь, литовский, гедимин, успешный,...</td>\n",
              "      <td>([кейстут, век, тракай, место, рождаться, окол...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   context_id   word  gold_sense_id  predict_sense_id  \\\n",
              "0           1  замок              1               NaN   \n",
              "1           2  замок              1               NaN   \n",
              "2           3  замок              1               NaN   \n",
              "3           4  замок              1               NaN   \n",
              "4           5  замок              1               NaN   \n",
              "\n",
              "                                             context  \\\n",
              "0  [владимир, мономах, любеч, многочисленный, укр...   \n",
              "1  [шильонский, шильйон, известный, русскоязычный...   \n",
              "2  [проведение, архитектурный, археологический, р...   \n",
              "3  [топь, белокуров, легенда, завещание, мавра, ю...   \n",
              "4  [великий, князь, литовский, гедимин, успешный,...   \n",
              "\n",
              "                                      embedding_need  \n",
              "0  ([ограда, строительство, лишь, часть, оборонит...  \n",
              "1  ([шильонский, разный, время, русскоязычный, эл...  \n",
              "2  ([музей, консультация, исторический, управлени...  \n",
              "3  ([топь, рождение, янтарный, мавра, завещание, ...  \n",
              "4  ([кейстут, век, тракай, место, рождаться, окол...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define necessary paths:\n",
        "path = \"russe-wsi-kit/data/main/\"\n",
        "\n",
        "wiki = \"wiki-wiki/\"\n",
        "bts = \"bts-rnc/\"\n",
        "actd = \"active-dict/\"\n",
        "\n",
        "train_file = \"train.csv\"\n",
        "test_file = \"test.csv\"\n",
        "\n",
        "\n",
        "# Prepare the data:\n",
        "wiki_train, wiki_train_global = read_preprocess(path + wiki + train_file)\n",
        "wiki_test, wiki_test_global = read_preprocess(path + wiki + test_file)\n",
        "\n",
        "bts_train, bts_train_global = read_preprocess(path + bts + train_file)\n",
        "bts_test, bts_test_global = read_preprocess(path + bts + test_file)\n",
        "\n",
        "actd_train, actd_train_global = read_preprocess(path + actd + train_file)\n",
        "actd_test, actd_test_global = read_preprocess(path + actd + test_file)\n",
        "\n",
        "wiki_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a",
      "metadata": {
        "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a"
      },
      "source": [
        "## 3.4 My method of text processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import(\n",
        "    AgglomerativeClustering,\n",
        "    AffinityPropagation,\n",
        "    KMeans,\n",
        "    SpectralClustering,\n",
        ")\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def give_emb_local_inv(model, context, counter, size, norm=False):\n",
        "    \"\"\"     \n",
        "        Calculates the context vector based on word embeddings and\n",
        "        each word is weighted by its relative frequency within one context. \n",
        "        \n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - context: list of words defining the context;\n",
        "            - counter: dictionary like structure, gives the number of times \n",
        "                a word was seen in this context by the key: model[key];\n",
        "            - size: size of word embeddings;\n",
        "            - norm: True - normalize, False - do not;\n",
        "        \n",
        "        Result: array\n",
        "    \"\"\"\n",
        "\n",
        "    if len(context) == 0:\n",
        "        return np.zeros(size)\n",
        "\n",
        "    weights = np.array([1 / counter[word] for word in context])\n",
        "    ov = np.average(model[context], axis=0, weights=weights)\n",
        "    if norm: \n",
        "        ov /= np.linalg.norm(ov)\n",
        "    return ov\n",
        "\n",
        "\n",
        "def give_emb_average(model, context, counter, size, norm=False):\n",
        "    \"\"\"     \n",
        "        Calculates the context vector based on average of word embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - context: list of words defining the context;\n",
        "            - counter: dictionary like structure, gives the number of times \n",
        "                a word was seen in this context by the key: model[key];\n",
        "            - size: size of word embeddings;\n",
        "            - norm: True - normalize, False - do not;\n",
        "        \n",
        "        Result: array  \n",
        "    \"\"\"\n",
        "\n",
        "    if len(context) == 0:\n",
        "        return np.zeros(size)\n",
        "\n",
        "    ov = model[context].mean(axis=0)\n",
        "    if norm: \n",
        "        ov /= np.linalg.norm(ov)\n",
        "    return ov\n",
        "\n",
        "\n",
        "def give_emb_sum(model, context, counter, size, norm=False):\n",
        "    \"\"\"     \n",
        "        Calculates the context vector based on sum of word embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - context: list of words defining the context;\n",
        "            - counter: dictionary like structure, gives the number of times \n",
        "                a word was seen in this context by the key: model[key];\n",
        "            - size: size of word embeddings;\n",
        "            - norm: True - normalize, False - do not;\n",
        "        \n",
        "        Result: array  \n",
        "    \"\"\"\n",
        "\n",
        "    if len(context) == 0:\n",
        "        return np.zeros(size)\n",
        "        \n",
        "    ov = model[context].sum(axis=0)\n",
        "    if norm: \n",
        "        ov /= np.linalg.norm(ov)\n",
        "    return ov\n",
        "\n",
        "\n",
        "def give_result(model, data, clus_alg, emb_alg, norm, train=True):\n",
        "    \"\"\"    \n",
        "        Calculates the context vector based on sum of word embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - data: Pandas Dataframe that has necessary columns:\n",
        "                \"embedding_need\", \"word\", \"gold_sense_id\";\n",
        "            - clus_alg: clustering function: e.g. 'Kmeans', 'SpectralClustering';\n",
        "            - emb_alg: algorithm to get context vector;\n",
        "            - norm: True - normalize, False - do not;\n",
        "            - train: True - evaluate with ARS, False - do not;\n",
        "        \n",
        "        Result: array  \n",
        "    \"\"\"\n",
        "\n",
        "    emb = np.array([emb_alg(model, context, counter, 300, norm) for context, counter in data[\"embedding_need\"]])\n",
        "    words = data[\"word\"].unique().tolist()\n",
        "    ars = []\n",
        "\n",
        "    labels = []\n",
        "    for i, word in enumerate(words):\n",
        "        mask = data[\"word\"] == word\n",
        "        clustering = clus_alg.fit(emb[mask])\n",
        "        labels += clustering.labels_.tolist()\n",
        "        if train:\n",
        "            ars.append(adjusted_rand_score(clustering.labels_, data.query(\"word == @word\")[\"gold_sense_id\"]))\n",
        "\n",
        "    return np.mean(ars), labels\n",
        "\n",
        "\n",
        "def give_2step_result(model, data, clus_alg, emb_alg, norm, pref=-2, train=True):\n",
        "    \"\"\"    \n",
        "        Calculates the context vector based on sum of word embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - data: Pandas Dataframe that has necessary columns:\n",
        "                \"embedding_need\", \"word\", \"gold_sense_id\";\n",
        "            - clus_alg: clustering function: e.g. 'Kmeans', 'SpectralClustering';\n",
        "            - emb_alg: algorithm to get context vector;\n",
        "            - norm: True - normalize, False - do not;\n",
        "            - pref: preference parameter for AffinityPropagation algorithm;\n",
        "            - train: True - evaluate with ARS, False - do not;\n",
        "        \n",
        "        Result: array  \n",
        "    \"\"\"\n",
        "\n",
        "    clus_find_n = AffinityPropagation(preference=pref, random_state=42)\n",
        "    emb = np.array([emb_alg(model, context, counter, 300, norm) for context, counter in data[\"embedding_need\"]])\n",
        "\n",
        "    words = data[\"word\"].unique().tolist()\n",
        "    ars = []\n",
        "\n",
        "    labels = []\n",
        "    for i, word in enumerate(words):\n",
        "        mask = data[\"word\"] == word\n",
        "        clustering = clus_find_n.fit(emb[mask])\n",
        "        clustering = clus_alg(len(set(clustering.labels_))).fit(emb[mask])\n",
        "        labels += clustering.labels_.tolist()\n",
        "        if train:\n",
        "            ars.append(adjusted_rand_score(clustering.labels_, data.query(\"word == @word\")[\"gold_sense_id\"]))\n",
        "\n",
        "    return np.mean(ars), labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WSIModel:\n",
        "    \"\"\" Word Sense Induction model based on pretrained word embeddings and clustering\"\"\"\n",
        "\n",
        "    def __init__(self, pretrained_model_path):\n",
        "        self.model = gensim.models.KeyedVectors.load(pretrained_model_path)\n",
        "        self.size = 300#self.model.\n",
        "        self.rs = 42\n",
        "        self.embedded = None\n",
        "        self.params = {\n",
        "            \"is_norm\": None,\n",
        "            \"emb_func\": None,\n",
        "            \"preference\": None,\n",
        "            \"clust_func\": None,\n",
        "        }\n",
        "        self.best_train_labels = None\n",
        "        self.test_labels = None       \n",
        "        self.global_dict = {}\n",
        "\n",
        "    def _update_params(self, is_norm, emb_func, pref, clust_func):\n",
        "        self.params[\"is_norm\"] = is_norm\n",
        "        self.params[\"emb_func\"] = emb_func\n",
        "        self.params[\"preference\"] = pref\n",
        "        self.params[\"clust_func\"] = clust_func\n",
        "\n",
        "    def _print_progess(self, emb_func, norm, clust_func, pref, ars_step, steps):\n",
        "        print(\n",
        "            f\"Embedding Method: {emb_func}; Is_norm = {norm}; \" \n",
        "            + f\"Clustering = {clust_func} ({steps} step); Preference = {pref}\"\n",
        "            + f\"\\n***Mean ARS = {ars_step}***\\n\"\n",
        "        )\n",
        "    \n",
        "    def _update_global_dict(self, data, labels):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def fit(self, train_data, nv=(True,), print_all=True, app=(-3, -2, -1)):\n",
        "        \"\"\"    \n",
        "            Fit the model parameters and returns train_data labels.\n",
        "\n",
        "            Parameters:\n",
        "                - train_data: Pandas Dataframe that has necessary columns:\n",
        "                    \"embedding_need\", \"word\", \"gold_sense_id\";\n",
        "                - nv: normalization parameters True/False ;\n",
        "                - print_all: print the results of every model to stdout;\n",
        "                - app: affinity propagation preference:\n",
        "                    iterable structure that has preferences for AP algorithm;\n",
        "        \n",
        "            Result: train_data clustering labels,\n",
        "                the order cooresponds to rows of train_data  \n",
        "        \"\"\"\n",
        "\n",
        "        embedding_funcs = (give_emb_local_inv, give_emb_average, give_emb_sum)\n",
        "        names_embedding_funcs = (\"Local_Average\", \"Average\", \"Sum\")\n",
        "\n",
        "        clustering_funcs = (\n",
        "            KMeans, AgglomerativeClustering, SpectralClustering,\n",
        "        )\n",
        "        names_clustering_funcs = (\n",
        "            \"KMeans\", \"AgglomerativeClustering\", \"SpectralClustering\",\n",
        "        )\n",
        "\n",
        "        norm_variants = nv\n",
        "        affinity_propagation_preference = app\n",
        "\n",
        "        best_train_labels = None\n",
        "        ars_init = 0.0\n",
        "        for is_norm in norm_variants:\n",
        "            for emb_func, name_emb_func in zip(embedding_funcs, names_embedding_funcs):\n",
        "                for pref in affinity_propagation_preference:\n",
        "                    for clust_func, name_clust_func in zip(clustering_funcs, names_clustering_funcs):\n",
        "                        \n",
        "                        # Get results for two step algorithm:\n",
        "                        ars_2step, labels_2step = give_2step_result(\n",
        "                            self.model,\n",
        "                            train_data,\n",
        "                            clust_func,\n",
        "                            emb_func,\n",
        "                            is_norm,\n",
        "                            pref,\n",
        "                        )\n",
        "                        if print_all:\n",
        "                            self._print_progess(\n",
        "                                name_emb_func, is_norm,\n",
        "                                name_clust_func, pref, ars_2step, 2,\n",
        "                            )\n",
        "                        \n",
        "                        if ars_init <= ars_2step:\n",
        "                            ars_init = ars_2step\n",
        "                            best_train_labels = labels_2step\n",
        "                            # Update parameters of the model:\n",
        "                            self._update_params(\n",
        "                                is_norm, emb_func,\n",
        "                                pref, clust_func,\n",
        "                            )\n",
        "                            self._print_progess(\n",
        "                                name_emb_func, is_norm,\n",
        "                                name_clust_func, pref, ars_2step, 2,\n",
        "                            )\n",
        "                    \n",
        "                    # Get results for one step algorithm:\n",
        "                    ars_1step, labels_1step = give_result(\n",
        "                            self.model,\n",
        "                            train_data,\n",
        "                            AffinityPropagation(preference=pref, random_state=self.rs),\n",
        "                            emb_func,\n",
        "                            is_norm,\n",
        "                    )\n",
        "                    if print_all:\n",
        "                        self._print_progess(\n",
        "                            name_emb_func, is_norm,\n",
        "                            \"AffinityPropagation\", pref, ars_2step, 1,\n",
        "                        )\n",
        "\n",
        "                    if ars_init <= ars_1step:\n",
        "                        ars_init = ars_1step\n",
        "                        best_train_labels = labels_1step\n",
        "                        # Update parameters of the model:\n",
        "                        self._update_params(\n",
        "                            is_norm, emb_func,\n",
        "                            pref, -1,\n",
        "                        )\n",
        "                        self._print_progess(\n",
        "                            name_emb_func, is_norm,\n",
        "                            \"AffinityPropagation\", pref, ars_2step, 1,\n",
        "                        )\n",
        "\n",
        "        self._update_global_dict(train_data, best_train_labels)\n",
        "        self.best_train_labels = best_train_labels\n",
        "        return best_train_labels\n",
        "\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        \"\"\"    \n",
        "            Predict the labels for test_data.\n",
        "\n",
        "            Parameters:\n",
        "                - test_data: Pandas Dataframe that has necessary columns:\n",
        "                    \"embedding_need\", \"word\";\n",
        "\n",
        "            Result: test_data clustering labels,\n",
        "                the order cooresponds to rows of test_data  \n",
        "        \"\"\"\n",
        "\n",
        "        if self.params[\"clust_func\"] == -1:\n",
        "            clust_func = AffinityPropagation(\n",
        "                preference=self.params[\"preference\"],\n",
        "                random_state=self.rs,\n",
        "            )\n",
        "            _, labels = give_result(\n",
        "                self.model,\n",
        "                test_data,\n",
        "                clust_func,\n",
        "                self.params[\"emb_func\"],\n",
        "                self.params[\"is_norm\"],\n",
        "                train=False\n",
        "            )\n",
        "        else:\n",
        "            _, labels = give_2step_result(\n",
        "                self.model,\n",
        "                test_data,\n",
        "                self.params[\"clust_func\"],\n",
        "                self.params[\"emb_func\"],\n",
        "                self.params[\"is_norm\"],\n",
        "                self.params[\"preference\"],\n",
        "                train=False,\n",
        "            )\n",
        "        \n",
        "        self._update_global_dict(test_data, labels)\n",
        "        self.test_labels = labels\n",
        "        return labels\n",
        "\n",
        "\n",
        "    def get_cluster_label(word, context):\n",
        "        \"\"\"Gives label by word and context list\"\"\"\n",
        "        \n",
        "        default_cluster = 0\n",
        "        counter = Counter(s)\n",
        "        cxt = list(set(context))\n",
        "        context_emb = self.params[\"emb_func\"](self.model, cxt, counter, self.size, self.params[\"is_norm\"])\n",
        "        \n",
        "        if word not in self.global_dict.keys():\n",
        "            self.global_dict[word] = {default_cluster:context_emb}\n",
        "            return default_cluster\n",
        "        \n",
        "        candidates = self.global_dict[word]\n",
        "        result = {clust_label: context_emb.dot() for clust_label, av_emb in candidates.items()}\n",
        "        return max(result.items(), key=operator.itemgetter(1))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Train and evaluate the models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols = [\"context_id\", \"word\", \"gold_sense_id\", \"predict_sense_id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## wiki-wiki dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.6919861201365022***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.7508223416134545***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.7851293039268663***\n",
            "\n",
            "Embedding Method: Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.7957682490549647***\n",
            "\n",
            "Embedding Method: Sum; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.7957682490549647***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train/Evaluate/Get test for CodaLab:\n",
        "\n",
        "wsi_model = WSIModel('ru_fasttext_model/model.model')\n",
        "wiki_train_labels = wsi_model.fit(wiki_train, print_all=False)\n",
        "wiki_test_labels = wsi_model.predict(wiki_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: test.csv (deflated 85%)\n"
          ]
        }
      ],
      "source": [
        "# Save the results:\n",
        "\n",
        "wiki_test[\"predict_sense_id\"] = wiki_test_labels\n",
        "wiki_test[cols].to_csv(\"test.csv\", sep='\\t', index=None)\n",
        "!zip wiki-wiki.zip test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bts-rnc dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Method: Local_Average; Is_norm = False; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.0***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = False; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.0***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = False; Clustering = AffinityPropagation (1 step); Preference = -3\n",
            "***Mean ARS = -0.0026280260061273316***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = False; Clustering = KMeans (2 step); Preference = -2\n",
            "***Mean ARS = 0.0***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = False; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.0***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = False; Clustering = SpectralClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.000710873017918456***\n",
            "\n",
            "Embedding Method: Average; Is_norm = False; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.08097254288020056***\n",
            "\n",
            "Embedding Method: Average; Is_norm = False; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.08447581910466799***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.10695325358028611***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.12202106591591551***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.12294224691023192***\n",
            "\n",
            "Embedding Method: Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -2\n",
            "***Mean ARS = 0.1253251866243145***\n",
            "\n",
            "Embedding Method: Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.12725871588006926***\n",
            "\n",
            "Embedding Method: Sum; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.12725871588006926***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train/Evaluate/Get test for CodaLab:\n",
        "\n",
        "wsi_model = WSIModel('ru_fasttext_model/model.model')\n",
        "bts_train_labels = wsi_model.fit(bts_train, nv=(False, True), print_all=False)\n",
        "bts_test_labels = wsi_model.predict(bts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: bts_test.csv (deflated 86%)\n"
          ]
        }
      ],
      "source": [
        "# Save the results:\n",
        "\n",
        "bts_test[\"predict_sense_id\"] = bts_test_labels\n",
        "bts_test[cols].to_csv(\"bts_test.csv\", sep='\\t', index=None)\n",
        "!zip bts.zip bts_test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## active-dict dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Method: Local_Average; Is_norm = False; Clustering = KMeans (2 step); Preference = -5\n",
            "***Mean ARS = 0.00019790524438484513***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = False; Clustering = AgglomerativeClustering (2 step); Preference = -5\n",
            "***Mean ARS = 0.00019790524438484513***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = False; Clustering = AffinityPropagation (1 step); Preference = -5\n",
            "***Mean ARS = -0.0013731907532188628***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = False; Clustering = SpectralClustering (2 step); Preference = -4\n",
            "***Mean ARS = 0.00506626049585486***\n",
            "\n",
            "Embedding Method: Average; Is_norm = False; Clustering = KMeans (2 step); Preference = -5\n",
            "***Mean ARS = 0.10785270897836366***\n",
            "\n",
            "Embedding Method: Average; Is_norm = False; Clustering = AgglomerativeClustering (2 step); Preference = -4\n",
            "***Mean ARS = 0.11175089441780814***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -1\n",
            "***Mean ARS = 0.23477566327576707***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = SpectralClustering (2 step); Preference = -1\n",
            "***Mean ARS = 0.23736261199417474***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train/Evaluate/Get test for CodaLab:\n",
        "\n",
        "wsi_model = WSIModel('ru_fasttext_model/model.model')\n",
        "actd_train_labels = wsi_model.fit(actd_train, nv=(False, True), print_all=False, app=(-5, -4, -3, -2, -1))\n",
        "actd_test_labels = wsi_model.predict(actd_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: actd.csv (deflated 85%)\n"
          ]
        }
      ],
      "source": [
        "# Save the results:\n",
        "\n",
        "actd_test[\"predict_sense_id\"] = actd_test_labels\n",
        "actd_test[cols].to_csv(\"actd.csv\", sep='\\t', index=None)\n",
        "!zip actd.zip actd.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.24682799, 0.41761101, 0.70951733, 0.59810252, 0.79420665],\n",
              "       [0.55941436, 0.87442077, 0.98456346, 0.81484982, 0.78967382],\n",
              "       [0.03175679, 0.13472281, 0.70276014, 0.7575814 , 0.75208123],\n",
              "       [0.79515903, 0.91340694, 0.3668862 , 0.84580481, 0.2110145 ]])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = np.random.rand(4, 5)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.15113538, 0.17845393, 0.25672481, 0.1982876 , 0.31182335],\n",
              "       [0.34253532, 0.37365829, 0.35624481, 0.27014534, 0.31004366],\n",
              "       [0.01944502, 0.05756988, 0.25427986, 0.25115927, 0.29528397],\n",
              "       [0.48688428, 0.3903179 , 0.13275052, 0.28040779, 0.08284903]])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a /= a.sum(axis=0)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.26850706, 0.27473166, 0.27124896, 0.25402907, 0.26200873])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.average(a, axis=0, weights=np.array([1, 2, 1, 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "34002bb53b7508659aadbc48d2690eaa0fedfc1e7a06e0392af55e4acdbf6811"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit ('snlp': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
