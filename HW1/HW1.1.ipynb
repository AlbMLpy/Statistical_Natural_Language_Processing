{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0",
      "metadata": {
        "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
      },
      "source": [
        "# 1. Information about the submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3",
      "metadata": {
        "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3"
      },
      "source": [
        "## 1.1 Name and number of the assignment "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9",
      "metadata": {
        "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9"
      },
      "source": [
        "## **Word Sense Induction(Knowledge-free)**, Assignment 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d",
      "metadata": {
        "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d"
      },
      "source": [
        "## 1.2 Student name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2",
      "metadata": {
        "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2"
      },
      "source": [
        "## **Albert Sayapin**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a46ab45-d215-41af-b910-63ff4a215a07",
      "metadata": {
        "id": "8a46ab45-d215-41af-b910-63ff4a215a07"
      },
      "source": [
        "## 1.3 Codalab user ID"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a",
      "metadata": {
        "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a"
      },
      "source": [
        "## **albertSayapin**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70456c74-8e1f-4da0-bebe-fbceee169115",
      "metadata": {
        "id": "70456c74-8e1f-4da0-bebe-fbceee169115"
      },
      "source": [
        "## 1.4 Additional comments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b810ac6-7739-4f7f-8bea-dbf1198570ea",
      "metadata": {
        "id": "6b810ac6-7739-4f7f-8bea-dbf1198570ea"
      },
      "source": [
        "## *This is a very cool task:)*\n",
        "\n",
        "Checked it on **Google Colab** successfully!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af498ab-3c00-4d36-a962-c947862fede8",
      "metadata": {
        "id": "1af498ab-3c00-4d36-a962-c947862fede8"
      },
      "source": [
        "# 2. Technical Report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061f71b9-114a-4cb0-b531-5711970317bf",
      "metadata": {
        "id": "061f71b9-114a-4cb0-b531-5711970317bf"
      },
      "source": [
        "## 2.1 Methodology "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3",
      "metadata": {
        "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3"
      },
      "source": [
        "The main problem I tried to solve is **Word Sense Induction**, meaning that we want to automatically discover the senses of semantically ambiguous words from unannotated text.\n",
        "\n",
        "Generally speaking, I used the model which consists of 3 components(Knowledge-Free):\n",
        "- precalculated words embeddings(to capture word semantics);\n",
        "- particular weighting scheme of embeddings(to approximate context semantics);\n",
        "- 1 step / 2 step clustering technique, namely how many clustering algorithms I use to get better results;\n",
        "\n",
        "Frankly, I was highly inspired by:\n",
        "- http://www.dialog-21.ru/media/4535/kutuzovab.pdf\n",
        "- http://www.dialog-21.ru/media/4538/arefyevn_ermolaevp_panchenkoa.pdf\n",
        "\n",
        "There are some essential steps in this project I had to walk through:\n",
        "\n",
        "1. **Data preprocessing**: I had to preprocess \"context\" column of every dataset(train/test):\n",
        "- *Lemmatized* all the words by pymystem3.Mystem stemmer and made them lowercase(Normalization step);\n",
        "- *Dropped* all the words from nltk *Russian stopwords* list(As they do not bring any additional information);\n",
        "- *Eliminated* all the words with *length* less than 3 and target word(\"word\" column) as well;\n",
        "- *Calculated words occurrences* both locally(for every context for every word) and globally(for every word) and left only unique tokens;\n",
        "(It could help to get more elaborate weighting method)\n",
        "\n",
        "2. **Model training**: I had to find optimal parameters for my model(using Brute Force as the size of the problem is not that big):\n",
        "- *pretrained word embeddings*: \n",
        "    - geowac_lemmas_none_fasttextskipgram_300_5_2020\n",
        "    - ruscorpora_upos_skipgram_300_5_2018\n",
        "- *weighting scheme*: Average, Sum, Local Sum;\n",
        "- *Normalize* context vectors or not?\n",
        "- *1 step or 2 step clustering*? (Used AffinityPropagation for the first step to identify the number of clusters)\n",
        "- What *clustering algorithm* to use? (AgglomerativeClustering, KMeans, SpectralClustering)\n",
        "\n",
        "3. **Model evaluation**: I had to use *Adjusted Rank Score* to compare different clusterings.\n",
        "\n",
        "4. **Send the results**: the test.csv -> .zip files to CodaLab system.\n",
        "\n",
        "*Some words about why I decided to move this particular way:*\n",
        "\n",
        "*First of all*, I used fastText Skipgram model that was precalculated on GeoWAC data. It is a good choice to get results quickly and with high accuracy(especially Gensim interface). \n",
        "\n",
        "*Secondly*, in order to represent context out of words you have to combine them in some way to get one representation.\n",
        "It is not pretty intuitive what method to use: just take the average or sum them up, weight all the words according to their relative frequency. Hence, I decided to look at them all and choose the best based on train ARS results.\n",
        "\n",
        "*Thirdly*, It is clustering of course. The challenging part I think, as we do not know how to define the number of clusters beforehand. Thanks to Affinity Propagation algorithm, it can find it almost automatically. After that we can just leave it as it is or use the next clustering algorithm that needs the number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe27e49-10c7-4c12-adea-48b0a05a5681",
      "metadata": {
        "id": "afe27e49-10c7-4c12-adea-48b0a05a5681"
      },
      "source": [
        "## 2.2 Discussion of results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b1c84c-c261-46b5-a009-0f2bc4002752",
      "metadata": {
        "id": "b5b1c84c-c261-46b5-a009-0f2bc4002752"
      },
      "source": [
        "### **Summary of the experiment:**\n",
        "Here below you can the best results I achieved with the help of two models:\n",
        "- **WSI18** -> which is based on *ruscorpora_upos_skipgram_300_5_2018*\n",
        "- **WSI20** -> which is based on *geowac_lemmas_none_fasttextskipgram_300_5_2020*\n",
        "\n",
        "### *Train.csv results:*\n",
        "\n",
        "Method | ARS_wiki-wiki | ARS_bts-rnc | ARS_active-dict|\n",
        "--- | --- | --- | --- |\n",
        "WSI18 | 0.796 | 0.127 | 0.241 |\n",
        "WSI20 | 0.819 | 0.144 | 0.303 |\n",
        "\n",
        "\n",
        "### *Test.csv results:*\n",
        "\n",
        "Method | ARS_wiki-wiki | ARS_bts-rnc | ARS_active-dict|\n",
        "--- | --- | --- | --- |\n",
        "Baseline(CodaLab) | 0.628 | 0.262 | 0.176 |\n",
        "WSI18 | 1.0 | 0.153 | 0.116 |\n",
        "WSI20 | 1.0 | 0.20 | 0.193 |\n",
        "\n",
        "\n",
        "### *Optimal parameters for the models:*\n",
        "Method | ARS_wiki-wiki | ARS_bts-rnc | ARS_active-dict|\n",
        "--- | --- | --- | --- |\n",
        "WSI18 | EM=S; N=1; C=AC; S=2; P=-3; | EM=S; N=1; C=AC; S=2; P=-2; | EM=W; N=1; C=KM; S=2; P=-1; |\n",
        "WSI20 | EM=S; N=1; C=AC; S=2; P=-3; | EM=W; N=1; C=AC; S=2; P=-2; | EM=S; N=1; C=SC; S=2; P=-1; |\n",
        "\n",
        "- EM = Embedding Method: {S: Sum; M: Mean; W: Weighted}\n",
        "- N = Normalization: {0: False; 1: True}\n",
        "- C = Clustering: {AC: AgglomerativeClustering; KM: KMeans; SC: SpectralClustering}\n",
        "- S = Number of steps: {1, 2}\n",
        "- P = Preference parameter for AffinityPropagation algorithm\n",
        "\n",
        "The tables show us that the model based on *geowac_lemmas_none_fasttextskipgram_300_5_2020* works better on every dataset with not so big differences in other parameters.\n",
        "Besides the method managed to beat the baseline on two datasets, excluding bts-rnc.\n",
        "The problem may be with chosen embeddings, as they could not capture the most valuable signal from provided contexts.\n",
        "That is why we can conclude that embeddings are a key component if we prefer to use this methodology.\n",
        "\n",
        "### **Conclusion:**\n",
        "As we can see from the results a model which is based on pretrained word embeddings can work pretty well.\n",
        "\n",
        "However the success and prediction accuracy depends highly on embeddings quality and applicability, weighting scheme and clustering method. \n",
        "\n",
        "That is why the more we improve these components the more accurate this particular model can become."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe",
      "metadata": {
        "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe"
      },
      "source": [
        "# 3. Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff93e37-3a24-40ab-87db-16b537aad3f6",
      "metadata": {
        "id": "dff93e37-3a24-40ab-87db-16b537aad3f6"
      },
      "source": [
        "## 3.1 Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "73daa932-114b-4e28-9141-13b57c729435",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73daa932-114b-4e28-9141-13b57c729435",
        "outputId": "73f8a3e2-d03f-43ce-9b34-6c6244c8daf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymystem3==0.1.10 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (0.1.10)\n",
            "Requirement already satisfied: requests in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from pymystem3==0.1.10) (2.26.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests->pymystem3==0.1.10) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests->pymystem3==0.1.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests->pymystem3==0.1.10) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests->pymystem3==0.1.10) (3.3)\n",
            "Requirement already satisfied: gensim==4.1.2 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (4.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gensim==4.1.2) (1.21.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gensim==4.1.2) (1.7.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gensim==4.1.2) (5.2.1)\n",
            "Requirement already satisfied: sklearn in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.7.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.21.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from scikit-learn->sklearn) (3.0.0)\n",
            "Requirement already satisfied: pandas in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (1.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from pandas) (2021.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from pandas) (1.21.4)\n",
            "Requirement already satisfied: six>=1.5 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
            "Requirement already satisfied: nltk in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (3.6.5)\n",
            "Requirement already satisfied: tqdm in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: joblib in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from nltk) (8.0.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from nltk) (2021.11.10)\n",
            "Requirement already satisfied: wget in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (3.2)\n",
            "Requirement already satisfied: gdown in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (4.2.0)\n",
            "Requirement already satisfied: filelock in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gdown) (3.4.0)\n",
            "Requirement already satisfied: requests[socks] in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gdown) (2.26.0)\n",
            "Requirement already satisfied: six in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from gdown) (4.10.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/albert/mambaforge/envs/snlp/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "--2021-11-27 22:23:33--  http://vectors.nlpl.eu/repository/20/214.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1920218982 (1,8G) [application/zip]\n",
            "Saving to: ‘214.zip’\n",
            "\n",
            "214.zip             100%[===================>]   1,79G  33,2MB/s    in 72s     \n",
            "\n",
            "2021-11-27 22:24:45 (25,6 MB/s) - ‘214.zip’ saved [1920218982/1920218982]\n",
            "\n",
            "Archive:  214.zip\n",
            "  inflating: ru_fasttext_model/meta.json  \n",
            "  inflating: ru_fasttext_model/model.model  \n",
            "  inflating: ru_fasttext_model/model.model.vectors_ngrams.npy  \n",
            "  inflating: ru_fasttext_model/model.model.vectors.npy  \n",
            "  inflating: ru_fasttext_model/model.model.vectors_vocab.npy  \n",
            "  inflating: ru_fasttext_model/README  \n",
            "--2021-11-27 22:24:57--  http://vectors.nlpl.eu/repository/20/213.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1485270300 (1,4G) [application/zip]\n",
            "Saving to: ‘213.zip’\n",
            "\n",
            "213.zip             100%[===================>]   1,38G  28,9MB/s    in 44s     \n",
            "\n",
            "2021-11-27 22:25:41 (32,1 MB/s) - ‘213.zip’ saved [1485270300/1485270300]\n",
            "\n",
            "Archive:  213.zip\n",
            "  inflating: new_model/meta.json     \n",
            "  inflating: new_model/model.model   \n",
            "  inflating: new_model/model.model.vectors_ngrams.npy  \n",
            "  inflating: new_model/model.model.vectors.npy  \n",
            "  inflating: new_model/model.model.vectors_vocab.npy  \n",
            "  inflating: new_model/README        \n"
          ]
        }
      ],
      "source": [
        "# Some essential packages:\n",
        "!pip install pymystem3==0.1.10\n",
        "!pip install gensim==4.1.2\n",
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install wget\n",
        "!pip install gdown\n",
        "\n",
        "# Embeddings for the model:\n",
        "!wget http://vectors.nlpl.eu/repository/20/214.zip\n",
        "!unzip -o 214.zip -d ru_fasttext_model\n",
        "!rm 214.zip\n",
        "\n",
        "!wget http://vectors.nlpl.eu/repository/20/213.zip\n",
        "!unzip -o 213.zip -d new_model\n",
        "!rm 213.zip\n",
        "\n",
        "!mkdir results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9",
      "metadata": {
        "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9"
      },
      "source": [
        "## 3.2 Download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "A5ptzz9XTxCL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ptzz9XTxCL",
        "outputId": "b856105b-074d-4803-c542-f20f6d8c8ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'russe-wsi-kit'...\n",
            "remote: Enumerating objects: 148, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 148 (delta 4), reused 22 (delta 4), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (148/148), 3.83 MiB | 4.70 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11nFGL1EJEgCr6VBVorhe5fnn3hnWd38r\n",
            "To: /home/albert/skoltech_classes/term2/snlp/project/Statistical_Natural_Language_Processing/HW1/wiki_test.csv\n",
            "100%|████████████████████████████████████████| 506k/506k [00:00<00:00, 4.73MB/s]\n"
          ]
        }
      ],
      "source": [
        "# To get train/test.csv:\n",
        "!git clone https://github.com/nlpub/russe-wsi-kit.git\n",
        "!gdown --id 11nFGL1EJEgCr6VBVorhe5fnn3hnWd38r\n",
        "!mv wiki_test.csv russe-wsi-kit/data/main/wiki-wiki/wiki_test.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791dc0e7-337d-46ad-96a3-543a732f19e2",
      "metadata": {
        "id": "791dc0e7-337d-46ad-96a3-543a732f19e2"
      },
      "source": [
        "## 3.3 Data Preprocessing: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "g3PpbSXNWyYP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3PpbSXNWyYP",
        "outputId": "8bc472aa-8d95-48d0-e104-357eb3c485fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/albert/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "\n",
        "def lemmatized_context(row, stemmer):\n",
        "    s = row['context']\n",
        "    target = row['word']\n",
        "    \n",
        "    tokens = stemmer.lemmatize(s.lower())\n",
        "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
        "                and token != \" \" \\\n",
        "                and re.match('[\\w\\-]+$', token)\\\n",
        "                and (len(token) > 2)\\\n",
        "                and token != target]\n",
        "    return tokens\n",
        "\n",
        "def get_counter(row):\n",
        "    s = row['context']\n",
        "    c = Counter(s)\n",
        "    return list(set(s)), c\n",
        "\n",
        "def get_counter_global(data):\n",
        "    return data.groupby(\"word\").apply(lambda x: Counter(x[\"context\"].sum()))\n",
        "\n",
        "def read_preprocess(data_path):\n",
        "    \n",
        "    # Read the data:\n",
        "    data = pd.read_csv(data_path, sep='\\t')\n",
        "\n",
        "    # Essential columns:\n",
        "    cols = [\n",
        "        'context_id', 'word', 'gold_sense_id',\n",
        "        'predict_sense_id', 'context',\n",
        "    ]\n",
        "\n",
        "    # Leave only essential columns:\n",
        "    data = data[cols]\n",
        "    \n",
        "    # Lemmatization:\n",
        "    stemmer = Mystem()\n",
        "    data['context'] = data.apply(lemmatized_context, 1, stemmer=stemmer)\n",
        "\n",
        "    # Leave unique and count all:\n",
        "    data[\"embedding_need\"] = data.apply(get_counter, axis=1)\n",
        "\n",
        "    # Calculate frequency based on all contexts:\n",
        "    word2glob_freq = get_counter_global(data).to_dict()\n",
        "\n",
        "    return data, word2glob_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "K0S-mez2oAKj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "K0S-mez2oAKj",
        "outputId": "53e18e18-c9a6-4fe9-cfb0-abd8523f9949"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context_id</th>\n",
              "      <th>word</th>\n",
              "      <th>gold_sense_id</th>\n",
              "      <th>predict_sense_id</th>\n",
              "      <th>context</th>\n",
              "      <th>embedding_need</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[владимир, мономах, любеч, многочисленный, укр...</td>\n",
              "      <td>([помимо, вышгородский, это, укреплять, также,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[шильонский, шильйон, известный, русскоязычный...</td>\n",
              "      <td>([располагать, представлять, известный, литера...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[проведение, архитектурный, археологический, р...</td>\n",
              "      <td>([потти, год, кальюнди, наука, называться, арх...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[топь, белокуров, легенда, завещание, мавра, ю...</td>\n",
              "      <td>([янушкевич, рождение, легенда, белокуров, ден...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>замок</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[великий, князь, литовский, гедимин, успешный,...</td>\n",
              "      <td>([год, второй, трок, правило, кейстут, литовск...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   context_id   word  gold_sense_id  predict_sense_id  \\\n",
              "0           1  замок              1               NaN   \n",
              "1           2  замок              1               NaN   \n",
              "2           3  замок              1               NaN   \n",
              "3           4  замок              1               NaN   \n",
              "4           5  замок              1               NaN   \n",
              "\n",
              "                                             context  \\\n",
              "0  [владимир, мономах, любеч, многочисленный, укр...   \n",
              "1  [шильонский, шильйон, известный, русскоязычный...   \n",
              "2  [проведение, архитектурный, археологический, р...   \n",
              "3  [топь, белокуров, легенда, завещание, мавра, ю...   \n",
              "4  [великий, князь, литовский, гедимин, успешный,...   \n",
              "\n",
              "                                      embedding_need  \n",
              "0  ([помимо, вышгородский, это, укреплять, также,...  \n",
              "1  ([располагать, представлять, известный, литера...  \n",
              "2  ([потти, год, кальюнди, наука, называться, арх...  \n",
              "3  ([янушкевич, рождение, легенда, белокуров, ден...  \n",
              "4  ([год, второй, трок, правило, кейстут, литовск...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define necessary paths:\n",
        "path = \"russe-wsi-kit/data/main/\"\n",
        "\n",
        "wiki = \"wiki-wiki/\"\n",
        "bts = \"bts-rnc/\"\n",
        "actd = \"active-dict/\"\n",
        "\n",
        "train_file = \"train.csv\"\n",
        "test_file = \"test.csv\"\n",
        "\n",
        "\n",
        "# Prepare the data:\n",
        "wiki_train, wiki_train_global = read_preprocess(path + wiki + train_file)\n",
        "wiki_test, wiki_test_global = read_preprocess(path + wiki + 'wiki_test.csv')\n",
        "\n",
        "bts_train, bts_train_global = read_preprocess(path + bts + train_file)\n",
        "bts_test, bts_test_global = read_preprocess(path + bts + test_file)\n",
        "\n",
        "actd_train, actd_train_global = read_preprocess(path + actd + train_file)\n",
        "actd_test, actd_test_global = read_preprocess(path + actd + test_file)\n",
        "\n",
        "wiki_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a",
      "metadata": {
        "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a"
      },
      "source": [
        "## 3.4 My method of text processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ocryOtixoAKj",
      "metadata": {
        "id": "ocryOtixoAKj"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import(\n",
        "    AgglomerativeClustering,\n",
        "    AffinityPropagation,\n",
        "    KMeans,\n",
        "    SpectralClustering,\n",
        ")\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def give_emb_local_inv(model, context, counter, size, norm=False):\n",
        "    \"\"\"     \n",
        "        Calculates the context vector based on word embeddings and\n",
        "        each word is weighted by its relative frequency within one context. \n",
        "        \n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - context: list of words defining the context;\n",
        "            - counter: dictionary like structure, gives the number of times \n",
        "                a word was seen in this context by the key: model[key];\n",
        "            - size: size of word embeddings;\n",
        "            - norm: True - normalize, False - do not;\n",
        "        \n",
        "        Result: array\n",
        "    \"\"\"\n",
        "\n",
        "    if len(context) == 0:\n",
        "        return np.zeros(size)\n",
        "\n",
        "    weights = np.array([1 / counter[word] for word in context])\n",
        "    ov = np.average(model[context], axis=0, weights=weights)\n",
        "    if norm: \n",
        "        ov /= np.linalg.norm(ov)\n",
        "    return ov\n",
        "\n",
        "\n",
        "def give_emb_average(model, context, counter, size, norm=False):\n",
        "    \"\"\"     \n",
        "        Calculates the context vector based on average of word embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - context: list of words defining the context;\n",
        "            - counter: dictionary like structure, gives the number of times \n",
        "                a word was seen in this context by the key: model[key];\n",
        "            - size: size of word embeddings;\n",
        "            - norm: True - normalize, False - do not;\n",
        "        \n",
        "        Result: array  \n",
        "    \"\"\"\n",
        "\n",
        "    if len(context) == 0:\n",
        "        return np.zeros(size)\n",
        "\n",
        "    ov = model[context].mean(axis=0)\n",
        "    if norm: \n",
        "        ov /= np.linalg.norm(ov)\n",
        "    return ov\n",
        "\n",
        "\n",
        "def give_emb_sum(model, context, counter, size, norm=False):\n",
        "    \"\"\"     \n",
        "        Calculates the context vector based on sum of word embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - context: list of words defining the context;\n",
        "            - counter: dictionary like structure, gives the number of times \n",
        "                a word was seen in this context by the key: model[key];\n",
        "            - size: size of word embeddings;\n",
        "            - norm: True - normalize, False - do not;\n",
        "        \n",
        "        Result: array  \n",
        "    \"\"\"\n",
        "\n",
        "    if len(context) == 0:\n",
        "        return np.zeros(size)\n",
        "        \n",
        "    ov = model[context].sum(axis=0)\n",
        "    if norm: \n",
        "        ov /= np.linalg.norm(ov)\n",
        "    return ov\n",
        "\n",
        "\n",
        "def get_centroids(embeddings, c_labels, data):\n",
        "    \"\"\"     \n",
        "        Calculates the centroids for particular word and labels.\n",
        "        (input is projected on particular word])\n",
        "\n",
        "        Parameters:\n",
        "            - embeddings: particular word context embeddings;\n",
        "            - c_labels: particular word context labels;\n",
        "            - data: Pandas Dataframe that has necessary columns:\n",
        "                \"word\", \"context;\n",
        "        \n",
        "        Result: dictionary: {label:(embedding, list of words for the context)}  \n",
        "    \"\"\"\n",
        "\n",
        "    centroids = {}\n",
        "    uniq_labels = np.unique(c_labels)\n",
        "\n",
        "    for label in uniq_labels:\n",
        "        label_mask = c_labels == label\n",
        "        ctx = data[\"context\"][label_mask].iloc[0]\n",
        "        centroids[label] = (np.mean(embeddings[label_mask], axis=0), ctx)\n",
        "        \n",
        "    return centroids\n",
        "\n",
        "\n",
        "def give_result(model, data, clus_alg, emb_alg, norm, train=True):\n",
        "    \"\"\"    \n",
        "        Calculates the context vector based on sum of word embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - data: Pandas Dataframe that has necessary columns:\n",
        "                \"embedding_need\", \"word\", \"gold_sense_id\";\n",
        "            - clus_alg: clustering function: e.g. 'Kmeans', 'SpectralClustering';\n",
        "            - emb_alg: algorithm to get context vector;\n",
        "            - norm: True - normalize, False - do not;\n",
        "            - train: True - evaluate with ARS, False - do not;\n",
        "        \n",
        "        Result: array  \n",
        "    \"\"\"\n",
        "\n",
        "    emb = np.array([emb_alg(model, context, counter, 300, norm) for context, counter in data[\"embedding_need\"]])\n",
        "    words = data[\"word\"].unique().tolist()\n",
        "    ars = []\n",
        "    labels = []\n",
        "    centroids = {}\n",
        "    for i, word in enumerate(words):\n",
        "        mask = data[\"word\"] == word\n",
        "        clustering = clus_alg.fit(emb[mask])\n",
        "        labels += clustering.labels_.tolist()\n",
        "        \n",
        "        if train:\n",
        "            ars.append(adjusted_rand_score(clustering.labels_, data.query(\"word == @word\")[\"gold_sense_id\"]))\n",
        "        \n",
        "        centroids[word] = get_centroids(emb[mask], clustering.labels_, data.query(\"word == @word\"))\n",
        "\n",
        "    return np.mean(ars), labels, centroids\n",
        "\n",
        "\n",
        "def give_2step_result(model, data, clus_alg, emb_alg, norm, pref=-2, train=True):\n",
        "    \"\"\"    \n",
        "        Calculates the context vector based on sum of word embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            - model: dictionary like structure, gives embedding by the word: model[word];\n",
        "            - data: Pandas Dataframe that has necessary columns:\n",
        "                \"embedding_need\", \"word\", \"gold_sense_id\";\n",
        "            - clus_alg: clustering function: e.g. 'Kmeans', 'SpectralClustering';\n",
        "            - emb_alg: algorithm to get context vector;\n",
        "            - norm: True - normalize, False - do not;\n",
        "            - pref: preference parameter for AffinityPropagation algorithm;\n",
        "            - train: True - evaluate with ARS, False - do not;\n",
        "        \n",
        "        Result: array  \n",
        "    \"\"\"\n",
        "    clust_func, name_clust_func = clus_alg \n",
        "    clus_find_n = AffinityPropagation(preference=pref, random_state=42)\n",
        "    emb = np.array([emb_alg(model, context, counter, 300, norm) for context, counter in data[\"embedding_need\"]])\n",
        "\n",
        "    words = data[\"word\"].unique().tolist()\n",
        "    ars = []\n",
        "    labels = []\n",
        "    centroids = {}\n",
        "    for i, word in enumerate(words):\n",
        "        parameter = {}\n",
        "        mask = data[\"word\"] == word\n",
        "        clustering = clus_find_n.fit(emb[mask])\n",
        "        parameter[\"n_clusters\"] = len(set(clustering.labels_))\n",
        "        if name_clust_func != \"AgglomerativeClustering\":\n",
        "            parameter[\"random_state\"] = 13\n",
        "        clustering = clust_func(**parameter).fit(emb[mask])\n",
        "        labels += clustering.labels_.tolist()\n",
        "        if train:\n",
        "            ars.append(adjusted_rand_score(clustering.labels_, data.query(\"word == @word\")[\"gold_sense_id\"]))\n",
        "        \n",
        "        centroids[word] = get_centroids(emb[mask], clustering.labels_, data.query(\"word == @word\"))\n",
        "\n",
        "    return np.mean(ars), labels, centroids\n",
        "\n",
        "def print_example(model):\n",
        "    words = list(model.global_dict.keys())\n",
        "    target_word = words[0]\n",
        "    context1 = model.global_dict[target_word][0][1]\n",
        "    context2 = model.global_dict[target_word][1][1]\n",
        "    print(\n",
        "        f\"Words in the system: {words}\\n\\nTarget word: {target_word}\"\n",
        "        + f\"\\n\\nFirst context sense : {context1}\\n\\nSecond context sense : {context2}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "kUqQhOtioAKk",
      "metadata": {
        "id": "kUqQhOtioAKk"
      },
      "outputs": [],
      "source": [
        "class WSIModel:\n",
        "    \"\"\" Word Sense Induction model based on pretrained word embeddings and clustering\"\"\"\n",
        "\n",
        "    def __init__(self, pretrained_model_path):\n",
        "        self.model = gensim.models.KeyedVectors.load(pretrained_model_path)\n",
        "        self.size = self.model.vector_size\n",
        "        self.rs = 42\n",
        "        self.embedded = None\n",
        "        self.params = {\n",
        "            \"is_norm\": None,\n",
        "            \"emb_func\": None,\n",
        "            \"preference\": None,\n",
        "            \"clust_func\": None,\n",
        "        }\n",
        "        self.best_train_labels = None\n",
        "        self.test_labels = None       \n",
        "        self.global_dict = {}\n",
        "\n",
        "    def _update_params(self, is_norm, emb_func, pref, clust_func):\n",
        "        self.params[\"is_norm\"] = is_norm\n",
        "        self.params[\"emb_func\"] = emb_func\n",
        "        self.params[\"preference\"] = pref\n",
        "        self.params[\"clust_func\"] = clust_func\n",
        "\n",
        "    def _print_progess(self, emb_func, norm, clust_func, pref, ars_step, steps):\n",
        "        print(\n",
        "            f\"Embedding Method: {emb_func}; Is_norm = {norm}; \" \n",
        "            + f\"Clustering = {clust_func} ({steps} step); Preference = {pref}\"\n",
        "            + f\"\\n***Mean ARS = {ars_step}***\\n\"\n",
        "        )\n",
        "    \n",
        "    def _update_global_dict(self, data, labels, centroids):\n",
        "        self.global_dict.update(centroids)\n",
        "\n",
        "\n",
        "    def fit(self, train_data, nv=(True,), print_all=True, app=(-3, -2, -1)):\n",
        "        \"\"\"    \n",
        "            Fit the model parameters and returns train_data labels.\n",
        "\n",
        "            Parameters:\n",
        "                - train_data: Pandas Dataframe that has necessary columns:\n",
        "                    \"embedding_need\", \"word\", \"gold_sense_id\";\n",
        "                - nv: normalization parameters True/False ;\n",
        "                - print_all: print the results of every model to stdout;\n",
        "                - app: affinity propagation preference:\n",
        "                    iterable structure that has preferences for AP algorithm;\n",
        "        \n",
        "            Result: train_data clustering labels,\n",
        "                the order cooresponds to rows of train_data  \n",
        "        \"\"\"\n",
        "\n",
        "        embedding_funcs = (give_emb_local_inv, give_emb_average, give_emb_sum)\n",
        "        names_embedding_funcs = (\"Local_Average\", \"Average\", \"Sum\")\n",
        "\n",
        "        clustering_funcs = (\n",
        "            KMeans, AgglomerativeClustering, SpectralClustering, \n",
        "        )\n",
        "        names_clustering_funcs = (\n",
        "            \"KMeans\", \"AgglomerativeClustering\", \"SpectralClustering\",\n",
        "        )\n",
        "\n",
        "        norm_variants = nv\n",
        "        affinity_propagation_preference = app\n",
        "\n",
        "        centroids = None\n",
        "        best_train_labels = None\n",
        "        ars_init = 0.0\n",
        "        for is_norm in norm_variants:\n",
        "            for emb_func, name_emb_func in zip(embedding_funcs, names_embedding_funcs):\n",
        "                for pref in affinity_propagation_preference:\n",
        "                    for clust_func, name_clust_func in zip(clustering_funcs, names_clustering_funcs):\n",
        "                        \n",
        "                        # Get results for two step algorithm:\n",
        "                        ars_2step, labels_2step, centroids2s = give_2step_result(\n",
        "                            self.model,\n",
        "                            train_data,\n",
        "                            (clust_func, name_clust_func),\n",
        "                            emb_func,\n",
        "                            is_norm,\n",
        "                            pref,\n",
        "                        )\n",
        "                        if print_all:\n",
        "                            self._print_progess(\n",
        "                                name_emb_func, is_norm,\n",
        "                                name_clust_func, pref, ars_2step, 2,\n",
        "                            )\n",
        "                        \n",
        "                        if ars_init <= ars_2step:\n",
        "                            ars_init = ars_2step\n",
        "                            best_train_labels = labels_2step\n",
        "                            # Update parameters of the model:\n",
        "                            self._update_params(\n",
        "                                is_norm, emb_func,\n",
        "                                pref, clust_func,\n",
        "                            )\n",
        "                            self._print_progess(\n",
        "                                name_emb_func, is_norm,\n",
        "                                name_clust_func, pref, ars_2step, 2,\n",
        "                            )\n",
        "                            centroids = centroids2s\n",
        "                    \n",
        "                    # Get results for one step algorithm:\n",
        "                    ars_1step, labels_1step, centroids1s = give_result(\n",
        "                            self.model,\n",
        "                            train_data,\n",
        "                            AffinityPropagation(preference=pref, random_state=self.rs),\n",
        "                            emb_func,\n",
        "                            is_norm,\n",
        "                    )\n",
        "                    if print_all:\n",
        "                        self._print_progess(\n",
        "                            name_emb_func, is_norm,\n",
        "                            \"AffinityPropagation\", pref, ars_2step, 1,\n",
        "                        )\n",
        "\n",
        "                    if ars_init <= ars_1step:\n",
        "                        ars_init = ars_1step\n",
        "                        best_train_labels = labels_1step\n",
        "                        # Update parameters of the model:\n",
        "                        self._update_params(\n",
        "                            is_norm, emb_func,\n",
        "                            pref, -1,\n",
        "                        )\n",
        "                        self._print_progess(\n",
        "                            name_emb_func, is_norm,\n",
        "                            \"AffinityPropagation\", pref, ars_2step, 1,\n",
        "                        )\n",
        "                        centroids = centroids1s\n",
        "\n",
        "        self._update_global_dict(train_data, best_train_labels, centroids)\n",
        "        self.best_train_labels = best_train_labels\n",
        "        return best_train_labels\n",
        "\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        \"\"\"    \n",
        "            Predict the labels for test_data.\n",
        "\n",
        "            Parameters:\n",
        "                - test_data: Pandas Dataframe that has necessary columns:\n",
        "                    \"embedding_need\", \"word\";\n",
        "\n",
        "            Result: test_data clustering labels,\n",
        "                the order cooresponds to rows of test_data  \n",
        "        \"\"\"\n",
        "        centroids = None\n",
        "        if self.params[\"clust_func\"] == -1:\n",
        "            clust_func = AffinityPropagation(\n",
        "                preference=self.params[\"preference\"],\n",
        "                random_state=self.rs,\n",
        "            )\n",
        "            _, labels, centroids = give_result(\n",
        "                self.model,\n",
        "                test_data,\n",
        "                clust_func,\n",
        "                self.params[\"emb_func\"],\n",
        "                self.params[\"is_norm\"],\n",
        "                train=False\n",
        "            )\n",
        "        else:\n",
        "            _, labels, centroids = give_2step_result(\n",
        "                self.model,\n",
        "                test_data,\n",
        "                (self.params[\"clust_func\"], self.params[\"clust_func\"].__name__),\n",
        "                self.params[\"emb_func\"],\n",
        "                self.params[\"is_norm\"],\n",
        "                self.params[\"preference\"],\n",
        "                train=False,\n",
        "            )\n",
        "        \n",
        "        self._update_global_dict(test_data, labels, centroids)\n",
        "        self.test_labels = labels\n",
        "        return labels\n",
        "\n",
        "\n",
        "    def get_cluster_label(word, context):\n",
        "        \"\"\"Gives label by word and context list\"\"\"\n",
        "        \n",
        "        default_cluster = 0\n",
        "        counter = Counter(s)\n",
        "        cxt = list(set(context))\n",
        "        context_emb = self.params[\"emb_func\"](self.model, cxt, counter, self.size, self.params[\"is_norm\"])\n",
        "        \n",
        "        if word not in self.global_dict.keys():\n",
        "            return None\n",
        "        \n",
        "        candidates = self.global_dict[word]\n",
        "        result = {clust_label: context_emb.dot() for clust_label, av_emb in candidates.items()}\n",
        "        return max(result.items(), key=operator.itemgetter(1))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gs82pxJPoAKr",
      "metadata": {
        "id": "gs82pxJPoAKr"
      },
      "source": [
        "## 3.5 Train and evaluate the models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fGpVv5EioAKs",
      "metadata": {
        "id": "fGpVv5EioAKs"
      },
      "outputs": [],
      "source": [
        "cols = [\"context_id\", \"word\", \"gold_sense_id\", \"predict_sense_id\"]\n",
        "emb_model_18 = 'ru_fasttext_model/model.model'\n",
        "emb_model_20 = 'new_model/model.model'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oVoPivjsoAKs",
      "metadata": {
        "id": "oVoPivjsoAKs"
      },
      "source": [
        "## *wiki-wiki dataset:*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mq4zaIQ5oAKs",
      "metadata": {
        "id": "mq4zaIQ5oAKs"
      },
      "source": [
        "### **ruscorpora_upos_skipgram_300_5_2018:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "RbGDQNUYoAKs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbGDQNUYoAKs",
        "outputId": "0cb55cba-9549-4ae6-f33c-61f37bfd7436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.6919861201365022***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.7508223416134545***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.7851293039268663***\n",
            "\n",
            "Embedding Method: Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.7957682490549647***\n",
            "\n",
            "Embedding Method: Sum; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.7957682490549647***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train/Evaluate/Get test for CodaLab:\n",
        "\n",
        "wsi_model1 = WSIModel(emb_model_18)\n",
        "wiki_train_labels1 = wsi_model1.fit(wiki_train, print_all=False)\n",
        "wiki_test_labels1 = wsi_model1.predict(wiki_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "R25WLLecoAKs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R25WLLecoAKs",
        "outputId": "0f077c72-bbe8-43f2-9ba1-6959e268436c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words in the system: ['замок', 'лук', 'суда', 'бор', 'банка', 'белка', 'бит', 'горе', 'граф', 'душ']\n",
            "\n",
            "Target word: замок\n",
            "\n",
            "First context sense : ['владимир', 'мономах', 'любеч', 'многочисленный', 'укреплять', 'монастырь', 'также', 'являться', 'таковой', 'это', 'крепость', 'ранний', 'европейский', 'строиться', 'преимущественно', 'дерево', 'опоясываться', 'деревянный', 'ограда', 'палисад', 'вокруг', 'становиться', 'появляться', 'ров', 'пример', 'мочь', 'служить', 'вышгородский', 'киевский', 'князь', 'каменный', 'замковый', 'строительство', 'распространяться', 'западный', 'центральный', 'европа', 'лишь', 'xii', 'век', 'главный', 'часть', 'средневековый', 'являться', 'центральный', 'башня', 'донжон', 'выполнять', 'функция', 'цитадель', 'помимо', 'свой', 'оборонительный', 'функция', 'донжон', 'являться', 'непосредственный', 'жилище', 'феодал', 'также', 'главный', 'башня']\n",
            "\n",
            "Second context sense : ['аркарт', 'открывать', 'протяжение', 'весь', 'год', 'часы', 'посещение', 'апрель', 'сентябрь', 'октябрь', 'март']\n"
          ]
        }
      ],
      "source": [
        "print_example(wsi_model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "nDPmPyAToAKs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDPmPyAToAKs",
        "outputId": "283d8dbe-2736-4fe8-d75f-29ffb71218c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: wiki.csv (deflated 85%)\n"
          ]
        }
      ],
      "source": [
        "# Save the results:\n",
        "\n",
        "wiki_test[\"predict_sense_id\"] = wiki_test_labels1\n",
        "wiki_test[cols].to_csv(\"wiki.csv\", sep='\\t', index=None)\n",
        "!zip results/wiki_wiki_18.zip wiki.csv\n",
        "!rm wiki.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cEPgSGjvoAKw",
      "metadata": {
        "id": "cEPgSGjvoAKw"
      },
      "source": [
        "### **geowac_lemmas_none_fasttextskipgram_300_5_2020:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "PIPqH1F6oAKw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIPqH1F6oAKw",
        "outputId": "18632a93-c450-48f2-fc42-452e46347674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.8189711565185964***\n",
            "\n",
            "Embedding Method: Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.8193169968725392***\n",
            "\n",
            "Embedding Method: Sum; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.8193169968725392***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train/Evaluate/Get test for CodaLab:\n",
        "\n",
        "wsi_model2 = WSIModel(emb_model_20)\n",
        "wiki_train_labels2 = wsi_model2.fit(wiki_train, print_all=False)\n",
        "wiki_test_labels2 = wsi_model2.predict(wiki_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Fsl8JURIoAKx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsl8JURIoAKx",
        "outputId": "a11839d0-3f52-481d-b997-9f76ae2962ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words in the system: ['замок', 'лук', 'суда', 'бор', 'банка', 'белка', 'бит', 'горе', 'граф', 'душ']\n",
            "\n",
            "Target word: замок\n",
            "\n",
            "First context sense : ['владимир', 'мономах', 'любеч', 'многочисленный', 'укреплять', 'монастырь', 'также', 'являться', 'таковой', 'это', 'крепость', 'ранний', 'европейский', 'строиться', 'преимущественно', 'дерево', 'опоясываться', 'деревянный', 'ограда', 'палисад', 'вокруг', 'становиться', 'появляться', 'ров', 'пример', 'мочь', 'служить', 'вышгородский', 'киевский', 'князь', 'каменный', 'замковый', 'строительство', 'распространяться', 'западный', 'центральный', 'европа', 'лишь', 'xii', 'век', 'главный', 'часть', 'средневековый', 'являться', 'центральный', 'башня', 'донжон', 'выполнять', 'функция', 'цитадель', 'помимо', 'свой', 'оборонительный', 'функция', 'донжон', 'являться', 'непосредственный', 'жилище', 'феодал', 'также', 'главный', 'башня']\n",
            "\n",
            "Second context sense : ['строение', 'калькировать', 'немой', 'slōʒ', 'средневерхненемецкий', 'слово', 'калькировать', 'запор', 'форт', 'укрепление']\n"
          ]
        }
      ],
      "source": [
        "print_example(wsi_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ti9NNIQ6oAKx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti9NNIQ6oAKx",
        "outputId": "f496f179-3d37-46ab-c348-ef288eefd8d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: wiki.csv (deflated 85%)\n"
          ]
        }
      ],
      "source": [
        "# Save the results:\n",
        "\n",
        "wiki_test[\"predict_sense_id\"] = wiki_test_labels2\n",
        "wiki_test[cols].to_csv(\"wiki.csv\", sep='\\t', index=None)\n",
        "!zip results/wiki_wiki_20.zip wiki.csv\n",
        "!rm wiki.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BU_Auws4oAKx",
      "metadata": {
        "id": "BU_Auws4oAKx"
      },
      "source": [
        "## *bts-rnc dataset:*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g_ieH27coAKx",
      "metadata": {
        "id": "g_ieH27coAKx"
      },
      "source": [
        "### **ruscorpora_upos_skipgram_300_5_2018:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "zZkeQMD9oAKx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZkeQMD9oAKx",
        "outputId": "46f85960-a9f6-4e68-dac2-8d28a8eaa35e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.11164311895880123***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.12202106591591551***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.12294224691023192***\n",
            "\n",
            "Embedding Method: Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -2\n",
            "***Mean ARS = 0.125985784596124***\n",
            "\n",
            "Embedding Method: Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.12725871588006926***\n",
            "\n",
            "Embedding Method: Sum; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.12725871588006926***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train/Evaluate/Get test for CodaLab:\n",
        "\n",
        "wsi_model1 = WSIModel(emb_model_18)\n",
        "bts_train_labels1 = wsi_model1.fit(bts_train, print_all=False)\n",
        "bts_test_labels1 = wsi_model1.predict(bts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "x77EiBwboAKz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x77EiBwboAKz",
        "outputId": "ee93ec03-7535-436e-8f8e-b57316d83583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words in the system: ['балка', 'вид', 'винт', 'горн', 'губа', 'жаба', 'клетка', 'крыло', 'купюра', 'курица', 'лавка', 'лайка', 'лев', 'лира', 'мина', 'мишень', 'обед', 'оклад', 'опушка', 'полис', 'пост', 'поток', 'проказа', 'пропасть', 'проспект', 'пытка', 'рысь', 'среда', 'хвост', 'штамп', 'акция', 'баба', 'байка', 'бум', 'бычок', 'вал', 'газ', 'гвоздика', 'гипербола', 'град', 'гусеница', 'дождь', 'домино', 'забой', 'икра', 'кабачок', 'капот', 'карьер', 'кличка', 'ключ', 'кок', 'кольцо', 'концерт', 'котелок', 'крона', 'круп', 'кулак', 'лейка', 'лук', 'мандарин', 'ножка', 'опора', 'патрон', 'печать', 'пол', 'полоз', 'почерк', 'пробка', 'рак', 'рок', 'свет', 'секрет', 'скат', 'слог', 'стан', 'стопка', 'таз', 'такса', 'тюрьма', 'шах', 'шашка']\n",
            "\n",
            "Target word: балка\n",
            "\n",
            "First context sense : ['пантюхин', 'склиф', 'выползать', 'улица', 'успевать', 'золотков', 'обрушиваться', 'душа', 'компания', 'парень', 'летний', 'геннадий']\n",
            "\n",
            "Second context sense : ['маленький', 'комната', 'очень', 'высокий', 'наклонять', 'голова', 'словно', 'подпирать', 'плечо', 'потолочный', 'посмотреть', 'сьянов', 'серьезный', 'черный', 'глаз', 'москва', 'испытывать']\n"
          ]
        }
      ],
      "source": [
        "print_example(wsi_model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "JeLCwQbVoAKz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeLCwQbVoAKz",
        "outputId": "674aa6cd-196e-44f0-8ad9-14f2f712b032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: bts.csv (deflated 86%)\n"
          ]
        }
      ],
      "source": [
        "# Save the results:\n",
        "\n",
        "bts_test[\"predict_sense_id\"] = bts_test_labels1\n",
        "bts_test[cols].to_csv(\"bts.csv\", sep='\\t', index=None)\n",
        "!zip results/bts_18.zip bts.csv\n",
        "!rm bts.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gaMowfzjoAKz",
      "metadata": {
        "id": "gaMowfzjoAKz"
      },
      "source": [
        "### **geowac_lemmas_none_fasttextskipgram_300_5_2020:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "hyA1pOtHoAKz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyA1pOtHoAKz",
        "outputId": "0c003b14-c6e2-4310-e851-7f146a608405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.08478654598201153***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.08840341279182526***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -2\n",
            "***Mean ARS = 0.1415032087485018***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.1446191574482564***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train/Evaluate/Get test for CodaLab:\n",
        "\n",
        "wsi_model2 = WSIModel(emb_model_20)\n",
        "bts_train_labels2 = wsi_model2.fit(bts_train, print_all=False)\n",
        "bts_test_labels2 = wsi_model2.predict(bts_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "WD3XxZnaoAKz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD3XxZnaoAKz",
        "outputId": "3d5d3e26-9d13-4405-8a36-61d3cee1054b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words in the system: ['балка', 'вид', 'винт', 'горн', 'губа', 'жаба', 'клетка', 'крыло', 'купюра', 'курица', 'лавка', 'лайка', 'лев', 'лира', 'мина', 'мишень', 'обед', 'оклад', 'опушка', 'полис', 'пост', 'поток', 'проказа', 'пропасть', 'проспект', 'пытка', 'рысь', 'среда', 'хвост', 'штамп', 'акция', 'баба', 'байка', 'бум', 'бычок', 'вал', 'газ', 'гвоздика', 'гипербола', 'град', 'гусеница', 'дождь', 'домино', 'забой', 'икра', 'кабачок', 'капот', 'карьер', 'кличка', 'ключ', 'кок', 'кольцо', 'концерт', 'котелок', 'крона', 'круп', 'кулак', 'лейка', 'лук', 'мандарин', 'ножка', 'опора', 'патрон', 'печать', 'пол', 'полоз', 'почерк', 'пробка', 'рак', 'рок', 'свет', 'секрет', 'скат', 'слог', 'стан', 'стопка', 'таз', 'такса', 'тюрьма', 'шах', 'шашка']\n",
            "\n",
            "Target word: балка\n",
            "\n",
            "First context sense : ['равнозначный', 'обеспечивать', 'меланхоличный', 'езда', 'цельнометаллический', 'тело', 'усиленный', 'передний', 'задний', 'поперечный', 'наряду', 'работа', 'подвеска', 'превращать', 'машина', 'глыба', 'весь', 'плоскость', 'врастать']\n",
            "\n",
            "Second context sense : ['маленький', 'комната', 'очень', 'высокий', 'наклонять', 'голова', 'словно', 'подпирать', 'плечо', 'потолочный', 'посмотреть', 'сьянов', 'серьезный', 'черный', 'глаз', 'москва', 'испытывать']\n"
          ]
        }
      ],
      "source": [
        "print_example(wsi_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3QilQTAToAKz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QilQTAToAKz",
        "outputId": "a6a0de8c-6b58-4f5f-da2e-b7695ddec681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: bts.csv (deflated 86%)\n"
          ]
        }
      ],
      "source": [
        "# Save the results:\n",
        "\n",
        "bts_test[\"predict_sense_id\"] = bts_test_labels2\n",
        "bts_test[cols].to_csv(\"bts.csv\", sep='\\t', index=None)\n",
        "!zip results/bts_20.zip bts.csv\n",
        "!rm bts.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "slz_e5DMoAKz",
      "metadata": {
        "id": "slz_e5DMoAKz"
      },
      "source": [
        "## *active-dict dataset:*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99gRtaRVoAKz",
      "metadata": {
        "id": "99gRtaRVoAKz"
      },
      "source": [
        "### **ruscorpora_upos_skipgram_300_5_2018:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "anq1MGPCoAKz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anq1MGPCoAKz",
        "outputId": "ce77724c-cff2-4cd9-8fd8-5929b3ce9ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.0030594407098070357***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.0034982330697388288***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = SpectralClustering (2 step); Preference = -3\n",
            "***Mean ARS = 0.003774388077124375***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -2\n",
            "***Mean ARS = 0.07108699785486196***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.07349557370249891***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = SpectralClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.08018192496429273***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -1\n",
            "***Mean ARS = 0.24114998545517347***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train/Evaluate/Get test for CodaLab:\n",
        "\n",
        "wsi_model1 = WSIModel(emb_model_18)\n",
        "actd_train_labels1 = wsi_model1.fit(actd_train, print_all=False)\n",
        "actd_test_labels1 = wsi_model1.predict(actd_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "xoWrpos5oAK0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoWrpos5oAK0",
        "outputId": "96f6e244-eaa2-4f39-ad10-82871c3d1e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words in the system: ['дар', 'двигатель', 'двойник', 'дворец', 'девятка', 'дедушка', 'дежурная', 'дежурный', 'декабрист', 'декрет', 'дело', 'демобилизация', 'демократ', 'демонстрация', 'дерево', 'держава', 'дерзость', 'десятка', 'десяток', 'деятель', 'диалог', 'диаметр', 'диплом', 'директор', 'диск', 'дичь', 'длина', 'доброволец', 'добыча', 'доказательство', 'доктор', 'долгота', 'доля', 'дом', 'дорога', 'достижение', 'древесина', 'дупло', 'дура', 'дух', 'дым', 'дымка', 'дыхание', 'дьявол', 'евро', 'езда', 'жаворонок', 'жало', 'жертва', 'жестокость', 'жидкость', 'жила', 'жилец', 'жир', 'жребий', 'заведение', 'завещание', 'зависимость', 'заголовок', 'заготовка', 'задание', 'задача', 'задержка', 'зажигалка', 'закон', 'закрытие', 'заложник', 'замена', 'западня', 'запятая', 'застой', 'затея', 'затишье', 'затмение', 'затруднение', 'захоронение', 'звезда', 'звон', 'зеркало', 'зло', 'злоупотребление', 'знак', 'знамя', 'значение', 'зонт', 'давление', 'дама', 'данные', 'дата', 'двойка', 'двор', 'дворник', 'девка', 'девочка', 'девушка', 'девчонка', 'дед', 'дезертир', 'действие', 'действительность', 'декларация', 'декорация', 'делегат', 'деление', 'дельфин', 'демократия', 'день', 'деньги', 'деревня', 'десант', 'десерт', 'деталь', 'детектив', 'дети', 'деятельность', 'диагональ', 'диво', 'диета', 'дикарь', 'диктатор', 'дипломатия', 'дискант', 'дистанция', 'дитя', 'дневник', 'днище', 'дно', 'добро', 'добродетель', 'доверие', 'дождь', 'доза', 'доклад', 'документ', 'долг', 'должник', 'доллар', 'дополнение', 'дорожка', 'досада', 'доска', 'достаток', 'достоинство', 'достояние', 'доступ', 'дочь', 'дощечка', 'драгоценность', 'драма', 'дробь', 'дрова', 'дрожжи', 'дрожь', 'дружба', 'дрянь', 'дуб', 'дубина', 'дуга', 'дурак', 'душ', 'душа', 'дуэт', 'дыня', 'дыра', 'дядя', 'дятел', 'евреи', 'еда', 'единица', 'единство', 'ежевика', 'ель', 'ельник', 'ерунда', 'жадность', 'жажда', 'жалоба', 'жалость', 'жар', 'железо', 'жемчуг', 'жена', 'жених', 'жест', 'жизнь', 'жилка', 'жук', 'журавль', 'журнал', 'жуть', 'забота', 'завеса', 'завивка', 'завиток', 'завоевание', 'завтрак', 'загадка', 'зад', 'зазор', 'заимствование', 'заказ', 'закат', 'закуска', 'зал', 'залог', 'замазка', 'заметка', 'замечание', 'замок', 'занавес', 'занятие', 'запас', 'записка', 'запись', 'заповедь', 'запрос', 'запруда', 'запуск', 'заработок', 'заражение', 'зараза', 'зародыш', 'заря', 'заряд', 'зарядка', 'засада', 'защита', 'защитник', 'заявка', 'заявление', 'заяц', 'звание', 'звено', 'зверство', 'зверь', 'звонок', 'звук', 'здоровье', 'зелень', 'земля', 'зенит', 'зерно', 'злодей', 'змея', 'знакомство', 'знание', 'значок', 'золото', 'зона', 'зонтик', 'зоология', 'зубр', 'зуд']\n",
            "\n",
            "Target word: дар\n",
            "\n",
            "First context sense : ['отвергать', 'щедрый']\n",
            "\n",
            "Second context sense : ['последний', 'изор']\n"
          ]
        }
      ],
      "source": [
        "print_example(wsi_model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "pFQr__jzoAK0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFQr__jzoAK0",
        "outputId": "7fe1420d-95c8-44f8-8e9a-b5121ded7d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: actd.csv (deflated 85%)\n"
          ]
        }
      ],
      "source": [
        "# Save the results:\n",
        "\n",
        "actd_test[\"predict_sense_id\"] = actd_test_labels1\n",
        "actd_test[cols].to_csv(\"actd.csv\", sep='\\t', index=None)\n",
        "!zip results/actd_18.zip actd.csv\n",
        "!rm actd.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8KfexpNoAK0",
      "metadata": {
        "id": "f8KfexpNoAK0"
      },
      "source": [
        "### **geowac_lemmas_none_fasttextskipgram_300_5_2020:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "kAzS53VNoAK0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAzS53VNoAK0",
        "outputId": "ae06cfca-57c9-41b9-925e-b6ea962a48c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -3\n",
            "***Mean ARS = 0.00996843744857003***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -2\n",
            "***Mean ARS = 0.07521435080938825***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = AgglomerativeClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.08899651456516436***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = SpectralClustering (2 step); Preference = -2\n",
            "***Mean ARS = 0.09119365037811128***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = KMeans (2 step); Preference = -1\n",
            "***Mean ARS = 0.2636972493668461***\n",
            "\n",
            "Embedding Method: Local_Average; Is_norm = True; Clustering = SpectralClustering (2 step); Preference = -1\n",
            "***Mean ARS = 0.29456321892257964***\n",
            "\n",
            "Embedding Method: Average; Is_norm = True; Clustering = SpectralClustering (2 step); Preference = -1\n",
            "***Mean ARS = 0.30311397841984955***\n",
            "\n",
            "Embedding Method: Sum; Is_norm = True; Clustering = SpectralClustering (2 step); Preference = -1\n",
            "***Mean ARS = 0.30311397841984955***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train/Evaluate/Get test for CodaLab:\n",
        "\n",
        "wsi_model2 = WSIModel(emb_model_20)\n",
        "actd_train_labels2 = wsi_model2.fit(actd_train, print_all=False)\n",
        "actd_test_labels2 = wsi_model2.predict(actd_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "G2Bz0lEqoAK0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Bz0lEqoAK0",
        "outputId": "80f2460a-bb6f-48f6-ddb6-93442060fcfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words in the system: ['дар', 'двигатель', 'двойник', 'дворец', 'девятка', 'дедушка', 'дежурная', 'дежурный', 'декабрист', 'декрет', 'дело', 'демобилизация', 'демократ', 'демонстрация', 'дерево', 'держава', 'дерзость', 'десятка', 'десяток', 'деятель', 'диалог', 'диаметр', 'диплом', 'директор', 'диск', 'дичь', 'длина', 'доброволец', 'добыча', 'доказательство', 'доктор', 'долгота', 'доля', 'дом', 'дорога', 'достижение', 'древесина', 'дупло', 'дура', 'дух', 'дым', 'дымка', 'дыхание', 'дьявол', 'евро', 'езда', 'жаворонок', 'жало', 'жертва', 'жестокость', 'жидкость', 'жила', 'жилец', 'жир', 'жребий', 'заведение', 'завещание', 'зависимость', 'заголовок', 'заготовка', 'задание', 'задача', 'задержка', 'зажигалка', 'закон', 'закрытие', 'заложник', 'замена', 'западня', 'запятая', 'застой', 'затея', 'затишье', 'затмение', 'затруднение', 'захоронение', 'звезда', 'звон', 'зеркало', 'зло', 'злоупотребление', 'знак', 'знамя', 'значение', 'зонт', 'давление', 'дама', 'данные', 'дата', 'двойка', 'двор', 'дворник', 'девка', 'девочка', 'девушка', 'девчонка', 'дед', 'дезертир', 'действие', 'действительность', 'декларация', 'декорация', 'делегат', 'деление', 'дельфин', 'демократия', 'день', 'деньги', 'деревня', 'десант', 'десерт', 'деталь', 'детектив', 'дети', 'деятельность', 'диагональ', 'диво', 'диета', 'дикарь', 'диктатор', 'дипломатия', 'дискант', 'дистанция', 'дитя', 'дневник', 'днище', 'дно', 'добро', 'добродетель', 'доверие', 'дождь', 'доза', 'доклад', 'документ', 'долг', 'должник', 'доллар', 'дополнение', 'дорожка', 'досада', 'доска', 'достаток', 'достоинство', 'достояние', 'доступ', 'дочь', 'дощечка', 'драгоценность', 'драма', 'дробь', 'дрова', 'дрожжи', 'дрожь', 'дружба', 'дрянь', 'дуб', 'дубина', 'дуга', 'дурак', 'душ', 'душа', 'дуэт', 'дыня', 'дыра', 'дядя', 'дятел', 'евреи', 'еда', 'единица', 'единство', 'ежевика', 'ель', 'ельник', 'ерунда', 'жадность', 'жажда', 'жалоба', 'жалость', 'жар', 'железо', 'жемчуг', 'жена', 'жених', 'жест', 'жизнь', 'жилка', 'жук', 'журавль', 'журнал', 'жуть', 'забота', 'завеса', 'завивка', 'завиток', 'завоевание', 'завтрак', 'загадка', 'зад', 'зазор', 'заимствование', 'заказ', 'закат', 'закуска', 'зал', 'залог', 'замазка', 'заметка', 'замечание', 'замок', 'занавес', 'занятие', 'запас', 'записка', 'запись', 'заповедь', 'запрос', 'запруда', 'запуск', 'заработок', 'заражение', 'зараза', 'зародыш', 'заря', 'заряд', 'зарядка', 'засада', 'защита', 'защитник', 'заявка', 'заявление', 'заяц', 'звание', 'звено', 'зверство', 'зверь', 'звонок', 'звук', 'здоровье', 'зелень', 'земля', 'зенит', 'зерно', 'злодей', 'змея', 'знакомство', 'знание', 'значок', 'золото', 'зона', 'зонтик', 'зоология', 'зубр', 'зуд']\n",
            "\n",
            "Target word: дар\n",
            "\n",
            "First context sense : ['отвергать', 'щедрый']\n",
            "\n",
            "Second context sense : ['мать', 'ревекка', 'приберегать', 'кусок', 'праздник', 'печь', 'калека', 'любимый', 'блюдо', 'сиять', 'гордость', 'бедный', 'бедный', 'богатство']\n"
          ]
        }
      ],
      "source": [
        "print_example(wsi_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "pOeTdHQAoAK0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOeTdHQAoAK0",
        "outputId": "dbad9691-6dc3-4642-dca6-0ccbc049067e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: actd.csv (deflated 85%)\n"
          ]
        }
      ],
      "source": [
        "# Save the results:\n",
        "\n",
        "actd_test[\"predict_sense_id\"] = actd_test_labels2\n",
        "actd_test[cols].to_csv(\"actd.csv\", sep='\\t', index=None)\n",
        "!zip results/actd_20.zip actd.csv\n",
        "!rm actd.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NV5eTs_800da",
      "metadata": {
        "id": "NV5eTs_800da"
      },
      "source": [
        "### extra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "T4NuXVno03IG",
      "metadata": {
        "id": "T4NuXVno03IG"
      },
      "outputs": [],
      "source": [
        "#!rm -fr results\n",
        "#!rm -fr ru_fasttext_model\n",
        "#!rm -fr new_model\n",
        "#!rm -fr russe-wsi-kit\n",
        "#!rm README "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "34002bb53b7508659aadbc48d2690eaa0fedfc1e7a06e0392af55e4acdbf6811"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit ('snlp': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
